{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Stage 5: Multi-Step Forecasting (6 time steps)\n",
    "\n",
    "**Building on Stage 4 Success:**\n",
    "- Stage 4: Single-step nowcasting (12 input → 1 output frame)\n",
    "- Results: CSI@74=0.82, CSI@181=0.50, CSI@219=0.33, LPIPS=0.137 ✅\n",
    "\n",
    "**Stage 5 Goal:** Extend to multi-step forecasting\n",
    "- Input: 12 frames (0-55 min of history)\n",
    "- Output: **6 frames** (predict 5, 10, 15, 20, 25, 30 min ahead)\n",
    "- Challenge: Maintain skill and sharpness across all lead times\n",
    "\n",
    "**Key Questions:**\n",
    "1. Does CSI degrade gracefully with lead time?\n",
    "2. Does blur accumulate (LPIPS increases)?\n",
    "3. Can we maintain extreme event skill (CSI@181, CSI@219)?\n",
    "4. Do we need perceptual loss to prevent blur?\n",
    "\n",
    "**Success Criteria:**\n",
    "- CSI@74 ≥ 0.70 at t+5min, ≥0.50 at t+30min\n",
    "- CSI@181 ≥ 0.40 at all lead times\n",
    "- LPIPS < 0.20 at all lead times (no blur accumulation)\n",
    "- Temporal consistency maintained\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Mount Drive & Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_DATA_ROOT = \"/content/drive/MyDrive/SEVIR_Data\"\n",
    "os.makedirs(DRIVE_DATA_ROOT, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Google Drive mounted\")\n",
    "print(f\"✓ Data directory: {DRIVE_DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"GPU CHECK\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: Select GPU runtime!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q h5py lpips tqdm matplotlib scikit-image pandas\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Setup - Use ALL 541 Events (Stage 4 Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = \"/content/drive/MyDrive/SEVIR_Data\"\n",
    "SEVIR_ROOT = f\"{DATA_ROOT}/data/sevir\"\n",
    "CATALOG_PATH = f\"{DATA_ROOT}/data/SEVIR_CATALOG.csv\"\n",
    "\n",
    "# Check data exists\n",
    "catalog_exists = Path(CATALOG_PATH).exists()\n",
    "vil_exists = Path(f\"{SEVIR_ROOT}/vil/2019/SEVIR_VIL_STORMEVENTS_2019_0701_1231.h5\").exists()\n",
    "\n",
    "print(f\"Data Check:\")\n",
    "print(f\"  Catalog: {'✓' if catalog_exists else '✗'} {CATALOG_PATH}\")\n",
    "print(f\"  VIL data: {'✓' if vil_exists else '✗'} {SEVIR_ROOT}/vil/2019/\")\n",
    "\n",
    "if not (catalog_exists and vil_exists):\n",
    "    print(\"\\n⚠ Data missing!\")\n",
    "else:\n",
    "    print(\"\\n✓ Data ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Event IDs (Same as Stage 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load catalog and get ALL VIL events\n",
    "catalog = pd.read_csv(CATALOG_PATH, low_memory=False)\n",
    "vil_catalog = catalog[catalog['img_type'] == 'vil'].copy()\n",
    "\n",
    "print(f\"Total VIL events in SEVIR: {len(vil_catalog)}\")\n",
    "\n",
    "# Get all unique event IDs\n",
    "all_event_ids = vil_catalog['id'].unique().tolist()\n",
    "print(f\"Unique events: {len(all_event_ids)}\")\n",
    "\n",
    "# Create 80/20 train/val split (same seed as Stage 4)\n",
    "np.random.seed(42)\n",
    "shuffled_ids = np.random.permutation(all_event_ids)\n",
    "\n",
    "n_train = int(len(all_event_ids) * 0.8)\n",
    "all_train_ids = shuffled_ids[:n_train].tolist()\n",
    "all_val_ids = shuffled_ids[n_train:].tolist()\n",
    "\n",
    "print(f\"\\n📊 Dataset Split:\")\n",
    "print(f\"  Train: {len(all_train_ids)} events\")\n",
    "print(f\"  Val: {len(all_val_ids)} events\")\n",
    "print(f\"  Total: {len(all_event_ids)} events\")\n",
    "\n",
    "# Save event ID files\n",
    "os.makedirs(f\"{DATA_ROOT}/data/samples\", exist_ok=True)\n",
    "\n",
    "TRAIN_IDS = f\"{DATA_ROOT}/data/samples/all_train_ids.txt\"\n",
    "VAL_IDS = f\"{DATA_ROOT}/data/samples/all_val_ids.txt\"\n",
    "\n",
    "with open(TRAIN_IDS, 'w') as f:\n",
    "    f.write('\\n'.join(all_train_ids))\n",
    "\n",
    "with open(VAL_IDS, 'w') as f:\n",
    "    f.write('\\n'.join(all_val_ids))\n",
    "\n",
    "print(f\"\\n✓ Event ID files ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Implementation (Multi-Step Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SevirMultiStepDataset(Dataset):\n",
    "    \"\"\"SEVIR VIL multi-step nowcasting dataset.\n",
    "    \n",
    "    Args:\n",
    "        index: List of (file_path, file_index, event_id) tuples\n",
    "        input_steps: Number of input frames (default: 12)\n",
    "        output_steps: Number of output frames to predict (default: 6)\n",
    "        target_size: Spatial size of frames\n",
    "    \"\"\"\n",
    "    def __init__(self, index, input_steps=12, output_steps=6, target_size=(384, 384)):\n",
    "        self.index = index\n",
    "        self.in_steps = input_steps\n",
    "        self.out_steps = output_steps\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, file_index, event_id = self.index[idx]\n",
    "\n",
    "        with h5py.File(file_path, \"r\") as h5:\n",
    "            data = h5[\"vil\"][file_index].astype(np.float32) / 255.0\n",
    "\n",
    "        total_frames = data.shape[2]\n",
    "        max_start = total_frames - (self.in_steps + self.out_steps)\n",
    "        t_start = np.random.randint(0, max(1, max_start + 1))\n",
    "\n",
    "        # Input: 12 frames\n",
    "        x = data[:, :, t_start:t_start + self.in_steps]\n",
    "        \n",
    "        # Output: 6 frames (next 6 time steps)\n",
    "        y = data[:, :, t_start + self.in_steps:t_start + self.in_steps + self.out_steps]\n",
    "\n",
    "        # Transpose to (T, H, W)\n",
    "        x = np.transpose(x, (2, 0, 1))\n",
    "        y = np.transpose(y, (2, 0, 1))\n",
    "\n",
    "        return torch.from_numpy(x).float(), torch.from_numpy(y).float()\n",
    "\n",
    "\n",
    "def build_index(catalog_path, ids_txt, sevir_root, modality=\"vil\"):\n",
    "    \"\"\"Build index from event ID file.\"\"\"\n",
    "    with open(ids_txt, 'r') as f:\n",
    "        event_ids = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    catalog = pd.read_csv(catalog_path, low_memory=False)\n",
    "    modality_cat = catalog[catalog[\"img_type\"] == modality].copy()\n",
    "\n",
    "    index = []\n",
    "    for event_id in event_ids:\n",
    "        event_rows = modality_cat[modality_cat[\"id\"] == event_id]\n",
    "        if event_rows.empty:\n",
    "            continue\n",
    "\n",
    "        row = event_rows.iloc[0]\n",
    "        file_path = os.path.join(sevir_root, row[\"file_name\"])\n",
    "        if os.path.exists(file_path):\n",
    "            index.append((file_path, int(row[\"file_index\"]), event_id))\n",
    "\n",
    "    print(f\"✓ Built index: {len(index)} events\")\n",
    "    return index\n",
    "\n",
    "print(\"✓ Multi-step dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model: UNet2D Adapted for Multi-Step Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import lpips\n",
    "\n",
    "# U-Net2D building blocks\n",
    "def conv_block(in_ch, out_ch, use_bn=True):\n",
    "    layers = [nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True)]\n",
    "    if use_bn: layers.append(nn.BatchNorm2d(out_ch))\n",
    "    layers += [nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True)]\n",
    "    if use_bn: layers.append(nn.BatchNorm2d(out_ch))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, use_bn=True):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.block = conv_block(in_ch, out_ch, use_bn)\n",
    "    def forward(self, x): return self.block(self.pool(x))\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, use_bn=True):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.block = conv_block(in_ch, out_ch, use_bn)\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        dy, dx = skip.size(-2) - x.size(-2), skip.size(-1) - x.size(-1)\n",
    "        if dy or dx:\n",
    "            x = F.pad(x, [dx//2, dx - dx//2, dy//2, dy - dy//2])\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        return self.block(x)\n",
    "\n",
    "class UNet2DMultiStep(nn.Module):\n",
    "    \"\"\"UNet2D adapted for multi-step forecasting.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Number of input time steps (default: 12)\n",
    "        out_channels: Number of output time steps (default: 6)\n",
    "        base_ch: Base number of channels (default: 32)\n",
    "        use_bn: Use batch normalization (default: True)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=12, out_channels=6, base_ch=32, use_bn=True):\n",
    "        super().__init__()\n",
    "        self.inc = conv_block(in_channels, base_ch, use_bn)\n",
    "        self.d1 = Down(base_ch, base_ch*2, use_bn)\n",
    "        self.d2 = Down(base_ch*2, base_ch*4, use_bn)\n",
    "        self.d3 = Down(base_ch*4, base_ch*8, use_bn)\n",
    "        self.bottleneck = conv_block(base_ch*8, base_ch*16, use_bn)\n",
    "        self.u3 = Up(base_ch*16, base_ch*8, use_bn)\n",
    "        self.u2 = Up(base_ch*8, base_ch*4, use_bn)\n",
    "        self.u1 = Up(base_ch*4, base_ch*2, use_bn)\n",
    "        self.u0 = Up(base_ch*2, base_ch, use_bn)\n",
    "        # Output 6 frames instead of 1\n",
    "        self.outc = nn.Conv2d(base_ch, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.inc(x)\n",
    "        c2 = self.d1(c1)\n",
    "        c3 = self.d2(c2)\n",
    "        c4 = self.d3(c3)\n",
    "        b  = self.bottleneck(c4)\n",
    "        x = self.u3(b, c4)\n",
    "        x = self.u2(x, c3)\n",
    "        x = self.u1(x, c2)\n",
    "        x = self.u0(x, c1)\n",
    "        return self.outc(x)  # Shape: (B, 6, H, W)\n",
    "\n",
    "# VGG Perceptual Loss (same as Stage 4)\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = torchvision.models.vgg16(weights='IMAGENET1K_V1').features\n",
    "        self.slice1 = nn.Sequential(*list(vgg[:4]))\n",
    "        self.slice2 = nn.Sequential(*list(vgg[4:9]))\n",
    "        self.slice3 = nn.Sequential(*list(vgg[9:16]))\n",
    "        self.slice4 = nn.Sequential(*list(vgg[16:23]))\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        if pred.shape[1] == 1:\n",
    "            pred = pred.repeat(1, 3, 1, 1)\n",
    "            target = target.repeat(1, 3, 1, 1)\n",
    "        pred = self.normalize(pred)\n",
    "        target = self.normalize(target)\n",
    "        \n",
    "        loss = 0\n",
    "        pred_1 = self.slice1(pred)\n",
    "        target_1 = self.slice1(target)\n",
    "        loss += F.mse_loss(pred_1, target_1)\n",
    "        \n",
    "        pred_2 = self.slice2(pred_1)\n",
    "        target_2 = self.slice2(target_1)\n",
    "        loss += F.mse_loss(pred_2, target_2)\n",
    "        \n",
    "        pred_3 = self.slice3(pred_2)\n",
    "        target_3 = self.slice3(target_2)\n",
    "        loss += F.mse_loss(pred_3, target_3)\n",
    "        \n",
    "        pred_4 = self.slice4(pred_3)\n",
    "        target_4 = self.slice4(target_3)\n",
    "        loss += F.mse_loss(pred_4, target_4)\n",
    "        \n",
    "        return loss / 4\n",
    "\n",
    "print(\"✓ Multi-step UNet2D model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Metrics (Per Lead Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIP_THRESHOLDS = [16, 74, 133, 160, 181, 219]\n",
    "\n",
    "def binarize(x, thr):\n",
    "    return (x >= thr/255.0).to(torch.int32)\n",
    "\n",
    "def scores(pred, truth, thresholds=VIP_THRESHOLDS):\n",
    "    \"\"\"Compute CSI, POD, SUCR, BIAS for single frame.\"\"\"\n",
    "    out = {}\n",
    "    for t in thresholds:\n",
    "        p = binarize(pred, t)\n",
    "        y = binarize(truth, t)\n",
    "        hits = ((p==1)&(y==1)).sum().item()\n",
    "        miss = ((p==0)&(y==1)).sum().item()\n",
    "        fa   = ((p==1)&(y==0)).sum().item()\n",
    "        pod = hits / (hits + miss + 1e-9)\n",
    "        sucr = hits / (hits + fa + 1e-9)\n",
    "        csi = hits / (hits + miss + fa + 1e-9)\n",
    "        bias = (hits + fa) / (hits + miss + 1e-9)\n",
    "        out[t] = dict(POD=pod, SUCR=sucr, CSI=csi, BIAS=bias)\n",
    "    return out\n",
    "\n",
    "def scores_per_lead_time(pred, truth, thresholds=VIP_THRESHOLDS):\n",
    "    \"\"\"Compute metrics for each of 6 lead times.\n",
    "    \n",
    "    Args:\n",
    "        pred: (B, 6, H, W) predictions\n",
    "        truth: (B, 6, H, W) ground truth\n",
    "    \n",
    "    Returns:\n",
    "        List of 6 dictionaries (one per lead time)\n",
    "    \"\"\"\n",
    "    lead_time_scores = []\n",
    "    for t in range(6):\n",
    "        pred_t = pred[:, t:t+1, :, :]  # (B, 1, H, W)\n",
    "        truth_t = truth[:, t:t+1, :, :]\n",
    "        lead_time_scores.append(scores(pred_t, truth_t, thresholds))\n",
    "    return lead_time_scores\n",
    "\n",
    "lpips_fn = lpips.LPIPS(net='alex').cuda() if torch.cuda.is_available() else lpips.LPIPS(net='alex')\n",
    "\n",
    "def compute_lpips(pred, target):\n",
    "    \"\"\"Compute LPIPS for single frame.\"\"\"\n",
    "    if pred.shape[1] == 1:\n",
    "        pred = pred.repeat(1, 3, 1, 1)\n",
    "        target = target.repeat(1, 3, 1, 1)\n",
    "    return lpips_fn(pred, target).mean().item()\n",
    "\n",
    "def compute_lpips_per_lead_time(pred, truth):\n",
    "    \"\"\"Compute LPIPS for each of 6 lead times.\"\"\"\n",
    "    lpips_scores = []\n",
    "    for t in range(6):\n",
    "        pred_t = pred[:, t:t+1, :, :]\n",
    "        truth_t = truth[:, t:t+1, :, :]\n",
    "        lpips_scores.append(compute_lpips(pred_t, truth_t))\n",
    "    return lpips_scores\n",
    "\n",
    "print(\"✓ Per-lead-time metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Function (Multi-Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import time\n",
    "\n",
    "def train_multistep_model(lambda_perc, epochs=10, batch_size=4, lr=1e-4, perceptual_scale=6000.0):\n",
    "    \"\"\"Train multi-step model with given lambda.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRAINING MULTI-STEP MODEL (Lambda = {lambda_perc})\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load datasets\n",
    "    train_index = build_index(CATALOG_PATH, TRAIN_IDS, SEVIR_ROOT, \"vil\")\n",
    "    val_index = build_index(CATALOG_PATH, VAL_IDS, SEVIR_ROOT, \"vil\")\n",
    "    \n",
    "    train_dataset = SevirMultiStepDataset(train_index, input_steps=12, output_steps=6)\n",
    "    val_dataset = SevirMultiStepDataset(val_index, input_steps=12, output_steps=6)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    # Create model (12 input frames → 6 output frames)\n",
    "    model = UNet2DMultiStep(in_channels=12, out_channels=6, base_ch=32, use_bn=True).to(device)\n",
    "    mse_criterion = nn.MSELoss()\n",
    "    perceptual_criterion = VGGPerceptualLoss().to(device) if lambda_perc > 0 else None\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = torch.amp.GradScaler('cuda') if device.type == 'cuda' else None\n",
    "    \n",
    "    # Enhanced history: track metrics PER LEAD TIME\n",
    "    history = {\n",
    "        'train_mse': [], 'train_perc': [], \n",
    "        'val_mse': [],\n",
    "    }\n",
    "    # Per lead time (t+5, t+10, t+15, t+20, t+25, t+30 min)\n",
    "    for t in range(6):\n",
    "        lead_min = (t+1) * 5\n",
    "        history[f'val_csi_74_t{lead_min}'] = []\n",
    "        history[f'val_csi_181_t{lead_min}'] = []\n",
    "        history[f'val_csi_219_t{lead_min}'] = []\n",
    "        history[f'val_lpips_t{lead_min}'] = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_mse = 0\n",
    "        train_perc = 0\n",
    "        \n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False):\n",
    "            x, y = x.to(device), y.to(device)  # x: (B, 12, H, W), y: (B, 6, H, W)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if scaler is not None:\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    pred = model(x)  # (B, 6, H, W)\n",
    "                    mse_loss = mse_criterion(pred, y)\n",
    "                    \n",
    "                    if lambda_perc > 0:\n",
    "                        # Compute perceptual loss averaged over all 6 frames\n",
    "                        perc_loss_total = 0\n",
    "                        for t in range(6):\n",
    "                            perc_loss_total += perceptual_criterion(pred[:, t:t+1], y[:, t:t+1])\n",
    "                        perc_loss = perc_loss_total / 6.0\n",
    "                        perc_scaled = perc_loss / perceptual_scale\n",
    "                        total = mse_loss + lambda_perc * perc_scaled\n",
    "                    else:\n",
    "                        perc_loss = torch.tensor(0.0)\n",
    "                        total = mse_loss\n",
    "                        \n",
    "                scaler.scale(total).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                pred = model(x)\n",
    "                mse_loss = mse_criterion(pred, y)\n",
    "                \n",
    "                if lambda_perc > 0:\n",
    "                    perc_loss_total = 0\n",
    "                    for t in range(6):\n",
    "                        perc_loss_total += perceptual_criterion(pred[:, t:t+1], y[:, t:t+1])\n",
    "                    perc_loss = perc_loss_total / 6.0\n",
    "                    perc_scaled = perc_loss / perceptual_scale\n",
    "                    total = mse_loss + lambda_perc * perc_scaled\n",
    "                else:\n",
    "                    perc_loss = torch.tensor(0.0)\n",
    "                    total = mse_loss\n",
    "                    \n",
    "                total.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_mse += mse_loss.item()\n",
    "            train_perc += perc_loss.item()\n",
    "        \n",
    "        train_mse /= len(train_loader)\n",
    "        train_perc /= len(train_loader)\n",
    "        \n",
    "        # Validate - Track metrics PER LEAD TIME\n",
    "        model.eval()\n",
    "        val_mse = 0\n",
    "        \n",
    "        # Accumulate per lead time\n",
    "        csi_per_lead = {t: {74: [], 181: [], 219: []} for t in range(6)}\n",
    "        lpips_per_lead = {t: [] for t in range(6)}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                pred = model(x)  # (B, 6, H, W)\n",
    "                val_mse += mse_criterion(pred, y).item()\n",
    "                \n",
    "                # Metrics per lead time\n",
    "                lead_scores = scores_per_lead_time(pred, y)\n",
    "                lead_lpips = compute_lpips_per_lead_time(pred, y)\n",
    "                \n",
    "                for t in range(6):\n",
    "                    csi_per_lead[t][74].append(lead_scores[t][74]['CSI'])\n",
    "                    csi_per_lead[t][181].append(lead_scores[t][181]['CSI'])\n",
    "                    csi_per_lead[t][219].append(lead_scores[t][219]['CSI'])\n",
    "                    lpips_per_lead[t].append(lead_lpips[t])\n",
    "        \n",
    "        val_mse /= len(val_loader)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_mse'].append(train_mse)\n",
    "        history['train_perc'].append(train_perc)\n",
    "        history['val_mse'].append(val_mse)\n",
    "        \n",
    "        for t in range(6):\n",
    "            lead_min = (t+1) * 5\n",
    "            history[f'val_csi_74_t{lead_min}'].append(np.mean(csi_per_lead[t][74]))\n",
    "            history[f'val_csi_181_t{lead_min}'].append(np.mean(csi_per_lead[t][181]))\n",
    "            history[f'val_csi_219_t{lead_min}'].append(np.mean(csi_per_lead[t][219]))\n",
    "            history[f'val_lpips_t{lead_min}'].append(np.mean(lpips_per_lead[t]))\n",
    "        \n",
    "        # Print progress (show t+5 and t+30 for comparison)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  MSE={val_mse:.4f}\")\n",
    "        print(f\"  t+5min:  CSI@74={history['val_csi_74_t5'][-1]:.3f}, CSI@181={history['val_csi_181_t5'][-1]:.3f}, LPIPS={history['val_lpips_t5'][-1]:.3f}\")\n",
    "        print(f\"  t+30min: CSI@74={history['val_csi_74_t30'][-1]:.3f}, CSI@181={history['val_csi_181_t30'][-1]:.3f}, LPIPS={history['val_lpips_t30'][-1]:.3f}\")\n",
    "    \n",
    "    # Save\n",
    "    os.makedirs('/content/outputs/checkpoints', exist_ok=True)\n",
    "    torch.save({'model': model.state_dict(), 'history': history, 'lambda': lambda_perc}, \n",
    "               f'/content/outputs/checkpoints/multistep_lambda{lambda_perc}.pt')\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"✓ Multi-step training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RUN TRAINING! 🚀\n",
    "\n",
    "**Strategy:**\n",
    "1. Test baseline (λ=0) first\n",
    "2. Check for blur accumulation across lead times\n",
    "3. If blur emerges, test with perceptual loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0.0, 0.0005]  # Start with baseline, optionally add perceptual\n",
    "results = {}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for lambda_val in lambdas:\n",
    "    history = train_multistep_model(lambda_val, epochs=10, batch_size=4)\n",
    "    \n",
    "    # Extract final metrics for each lead time\n",
    "    results[lambda_val] = {'history': history}\n",
    "    for t in range(6):\n",
    "        lead_min = (t+1) * 5\n",
    "        results[lambda_val][f't{lead_min}'] = {\n",
    "            'csi_74': max(history[f'val_csi_74_t{lead_min}']),\n",
    "            'csi_181': max(history[f'val_csi_181_t{lead_min}']),\n",
    "            'csi_219': max(history[f'val_csi_219_t{lead_min}']),\n",
    "            'lpips': min(history[f'val_lpips_t{lead_min}'])\n",
    "        }\n",
    "    \n",
    "    # Early analysis for baseline\n",
    "    if lambda_val == 0.0:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"BASELINE (λ=0) ANALYSIS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Check skill degradation\n",
    "        csi_74_t5 = results[0.0]['t5']['csi_74']\n",
    "        csi_74_t30 = results[0.0]['t30']['csi_74']\n",
    "        degradation = ((csi_74_t5 - csi_74_t30) / csi_74_t5) * 100\n",
    "        \n",
    "        print(f\"\\nSkill Degradation (CSI@74):\")\n",
    "        print(f\"  t+5min:  {csi_74_t5:.3f}\")\n",
    "        print(f\"  t+30min: {csi_74_t30:.3f}\")\n",
    "        print(f\"  Degradation: {degradation:.1f}%\")\n",
    "        \n",
    "        # Check blur accumulation\n",
    "        lpips_t5 = results[0.0]['t5']['lpips']\n",
    "        lpips_t30 = results[0.0]['t30']['lpips']\n",
    "        blur_increase = ((lpips_t30 - lpips_t5) / lpips_t5) * 100\n",
    "        \n",
    "        print(f\"\\nBlur Accumulation (LPIPS):\")\n",
    "        print(f\"  t+5min:  {lpips_t5:.3f}\")\n",
    "        print(f\"  t+30min: {lpips_t30:.3f}\")\n",
    "        print(f\"  Increase: {blur_increase:.1f}%\")\n",
    "        \n",
    "        # Check extreme event skill\n",
    "        csi_181_t30 = results[0.0]['t30']['csi_181']\n",
    "        print(f\"\\nExtreme Event Skill at t+30min:\")\n",
    "        print(f\"  CSI@181: {csi_181_t30:.3f} (target: ≥0.40)\")\n",
    "        \n",
    "        if lpips_t30 > 0.20:\n",
    "            print(f\"\\n⚠️  BLUR ACCUMULATION DETECTED!\")\n",
    "            print(f\"   → Perceptual loss recommended for next experiment\")\n",
    "        else:\n",
    "            print(f\"\\n✅ SHARPNESS MAINTAINED!\")\n",
    "            print(f\"   → Pure MSE sufficient for multi-step\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n✅ TRAINING COMPLETE in {total_time/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Detailed Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-STEP RESULTS: Per Lead Time Analysis\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for lambda_val in results:\n",
    "    print(f\"\\nLambda = {lambda_val}:\")\n",
    "    print(f\"{'Lead Time':<12} {'CSI@74':<10} {'CSI@181':<10} {'CSI@219':<10} {'LPIPS':<10}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for t in range(6):\n",
    "        lead_min = (t+1) * 5\n",
    "        res = results[lambda_val][f't{lead_min}']\n",
    "        print(f\"t+{lead_min}min{' '*6} {res['csi_74']:<10.3f} {res['csi_181']:<10.3f} {res['csi_219']:<10.3f} {res['lpips']:<10.3f}\")\n",
    "\n",
    "# Success evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUCCESS CRITERIA CHECK (Lambda=0.0)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline = results[0.0]\n",
    "checks = [\n",
    "    (\"CSI@74 ≥ 0.70 at t+5min\", baseline['t5']['csi_74'] >= 0.70, baseline['t5']['csi_74']),\n",
    "    (\"CSI@74 ≥ 0.50 at t+30min\", baseline['t30']['csi_74'] >= 0.50, baseline['t30']['csi_74']),\n",
    "    (\"CSI@181 ≥ 0.40 at t+30min\", baseline['t30']['csi_181'] >= 0.40, baseline['t30']['csi_181']),\n",
    "    (\"LPIPS < 0.20 at all times\", all(baseline[f't{(t+1)*5}']['lpips'] < 0.20 for t in range(6)), \"See table\")\n",
    "]\n",
    "\n",
    "all_pass = True\n",
    "for criterion, passed, value in checks:\n",
    "    status = \"✅ PASS\" if passed else \"❌ FAIL\"\n",
    "    print(f\"{criterion:<40} {status:>10} (value: {value})\")\n",
    "    if not passed:\n",
    "        all_pass = False\n",
    "\n",
    "if all_pass:\n",
    "    print(\"\\n🎉 STAGE 5 SUCCESS! All criteria met.\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Some criteria not met - consider architectural changes or perceptual loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualization: Skill Degradation Over Lead Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 0.0 in results:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    lead_times = [5, 10, 15, 20, 25, 30]\n",
    "    \n",
    "    # Plot 1: CSI degradation\n",
    "    csi_74 = [results[0.0][f't{t}']['csi_74'] for t in lead_times]\n",
    "    csi_181 = [results[0.0][f't{t}']['csi_181'] for t in lead_times]\n",
    "    csi_219 = [results[0.0][f't{t}']['csi_219'] for t in lead_times]\n",
    "    \n",
    "    axes[0].plot(lead_times, csi_74, 'o-', label='CSI@74 (Moderate)', linewidth=2)\n",
    "    axes[0].plot(lead_times, csi_181, 's-', label='CSI@181 (Extreme)', linewidth=2)\n",
    "    axes[0].plot(lead_times, csi_219, '^-', label='CSI@219 (Hail)', linewidth=2)\n",
    "    axes[0].axhline(y=0.50, color='green', linestyle='--', alpha=0.5, label='Target (0.50)')\n",
    "    axes[0].axhline(y=0.40, color='orange', linestyle='--', alpha=0.5, label='Min Extreme (0.40)')\n",
    "    axes[0].set_xlabel('Lead Time (minutes)')\n",
    "    axes[0].set_ylabel('CSI')\n",
    "    axes[0].set_title('Forecast Skill vs Lead Time (λ=0)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 2: LPIPS (sharpness) over lead time\n",
    "    lpips_vals = [results[0.0][f't{t}']['lpips'] for t in lead_times]\n",
    "    axes[1].plot(lead_times, lpips_vals, 'o-', linewidth=2, color='purple')\n",
    "    axes[1].axhline(y=0.20, color='red', linestyle='--', alpha=0.5, label='Blur threshold (0.20)')\n",
    "    axes[1].axhline(y=0.137, color='green', linestyle='--', alpha=0.5, label='Stage 4 baseline (0.137)')\n",
    "    axes[1].set_xlabel('Lead Time (minutes)')\n",
    "    axes[1].set_ylabel('LPIPS (lower = sharper)')\n",
    "    axes[1].set_title('Sharpness vs Lead Time (λ=0)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Compare Stage 4 vs Stage 5 (t+5min)\n",
    "    categories = ['CSI@74', 'CSI@181', 'CSI@219']\n",
    "    stage4 = [0.818, 0.499, 0.334]  # From Stage 4 results\n",
    "    stage5_t5 = [results[0.0]['t5']['csi_74'], results[0.0]['t5']['csi_181'], results[0.0]['t5']['csi_219']]\n",
    "    stage5_t30 = [results[0.0]['t30']['csi_74'], results[0.0]['t30']['csi_181'], results[0.0]['t30']['csi_219']]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.25\n",
    "    \n",
    "    axes[2].bar(x - width, stage4, width, label='Stage 4 (1-step)', alpha=0.8)\n",
    "    axes[2].bar(x, stage5_t5, width, label='Stage 5 (t+5min)', alpha=0.8)\n",
    "    axes[2].bar(x + width, stage5_t30, width, label='Stage 5 (t+30min)', alpha=0.8)\n",
    "    axes[2].set_ylabel('CSI')\n",
    "    axes[2].set_title('Stage 4 vs Stage 5 Performance')\n",
    "    axes[2].set_xticks(x)\n",
    "    axes[2].set_xticklabels(categories)\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/drive/MyDrive/stormfusion_results/stage5_multistep_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Analysis plots saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /content/drive/MyDrive/stormfusion_results/stage5_multistep\n",
    "!cp -r /content/outputs/checkpoints/* /content/drive/MyDrive/stormfusion_results/stage5_multistep/\n",
    "\n",
    "summary = f\"\"\"Stage 5 Results - Multi-Step Forecasting\n",
    "==========================================\n",
    "Dataset: {len(all_train_ids)} train / {len(all_val_ids)} val events\n",
    "Architecture: UNet2D (12 input frames → 6 output frames)\n",
    "Total Time: {total_time/60:.1f} min\n",
    "\n",
    "BASELINE (λ=0.0) RESULTS:\n",
    "-------------------------\n",
    "\"\"\"\n",
    "\n",
    "for t in range(6):\n",
    "    lead_min = (t+1) * 5\n",
    "    res = results[0.0][f't{lead_min}']\n",
    "    summary += f\"\\nt+{lead_min}min: CSI@74={res['csi_74']:.3f}, CSI@181={res['csi_181']:.3f}, CSI@219={res['csi_219']:.3f}, LPIPS={res['lpips']:.3f}\"\n",
    "\n",
    "summary += \"\\n\\nKEY INSIGHTS:\\n\"\n",
    "baseline = results[0.0]\n",
    "degradation = ((baseline['t5']['csi_74'] - baseline['t30']['csi_74']) / baseline['t5']['csi_74']) * 100\n",
    "summary += f\"\\nSkill degradation (CSI@74): {degradation:.1f}% from t+5 to t+30\"\n",
    "\n",
    "lpips_change = ((baseline['t30']['lpips'] - baseline['t5']['lpips']) / baseline['t5']['lpips']) * 100\n",
    "summary += f\"\\nBlur accumulation (LPIPS): {lpips_change:.1f}% from t+5 to t+30\"\n",
    "\n",
    "with open('/content/drive/MyDrive/stormfusion_results/stage5_multistep/summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"✅ Results saved to Drive!\")\n",
    "print(f\"   Location: /content/drive/MyDrive/stormfusion_results/stage5_multistep/\")\n",
    "print(f\"\\n{summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Stage 5 Complete!\n",
    "\n",
    "**What We Tested:**\n",
    "- Multi-step forecasting (1→6 frames)\n",
    "- Skill degradation over lead time\n",
    "- Blur accumulation (LPIPS trend)\n",
    "- Extreme event skill at longer horizons\n",
    "\n",
    "**Expected Outcomes:**\n",
    "\n",
    "1. **If sharpness maintained (LPIPS < 0.20 at all times):**\n",
    "   - ✅ Pure MSE sufficient for multi-step\n",
    "   - Ready for Stage 6 (Generative models)\n",
    "\n",
    "2. **If blur accumulates (LPIPS > 0.20 at t+30):**\n",
    "   - Add perceptual loss (λ≈0.0005)\n",
    "   - Helps maintain sharpness over time\n",
    "\n",
    "**Next Steps:**\n",
    "- Review per-lead-time metrics\n",
    "- Check analysis plots\n",
    "- Proceed to Stage 6 (GANs/Diffusion for probabilistic forecasting)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
