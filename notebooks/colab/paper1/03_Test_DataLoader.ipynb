{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03: Test Data Loader\n",
    "\n",
    "**Purpose:** Test the dataset and data loading pipeline\n",
    "\n",
    "**What this does:**\n",
    "- Load event IDs\n",
    "- Build dataset index\n",
    "- Test loading a few samples\n",
    "- Verify data shapes and ranges\n",
    "- Test augmentation\n",
    "- Test batching\n",
    "\n",
    "**What this does NOT do:**\n",
    "- Create models\n",
    "- Run training\n",
    "- Load all data (only a few samples for testing)\n",
    "\n",
    "**Expected time:** 2-3 minutes\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Run `01_Setup_and_Environment.ipynb`\n",
    "2. Run `02_Data_Verification.ipynb` and ensure data exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\nimport sys\nimport os\nimport torch\n\n# Mount Drive\nprint(\"Mounting Google Drive...\")\ndrive.mount('/content/drive', force_remount=False)\nprint(\"✅ Drive mounted\\n\")\n\n# Clone/update repository\nREPO_PATH = '/content/stormfusion-sevir'\nif not os.path.exists(REPO_PATH):\n    print(\"Cloning repository...\")\n    !git clone https://github.com/syedhaliz/stormfusion-sevir.git {REPO_PATH}\n    print(\"✅ Repository cloned\\n\")\nelse:\n    print(\"Repository exists, pulling latest changes...\")\n    !cd {REPO_PATH} && git pull\n    print(\"✅ Repository updated\\n\")\n\n# Add repo to path\nif REPO_PATH not in sys.path:\n    sys.path.insert(0, REPO_PATH)\n    print(f\"✅ Added {REPO_PATH} to Python path\\n\")\n\n# Force reload of modules to get latest code\nimport importlib\nif 'stormfusion.data.sevir_multimodal' in sys.modules:\n    print(\"Reloading sevir_multimodal module to get latest code...\")\n    importlib.reload(sys.modules['stormfusion.data.sevir_multimodal'])\n    print(\"✅ Module reloaded\\n\")\n\n# Paths\nDRIVE_ROOT = \"/content/drive/MyDrive/SEVIR_Data\"\nSEVIR_ROOT = f\"{DRIVE_ROOT}/data/sevir\"\nCATALOG_PATH = f\"{DRIVE_ROOT}/data/SEVIR_CATALOG.csv\"\nTRAIN_IDS = f\"{DRIVE_ROOT}/data/samples/all_train_ids.txt\"\nVAL_IDS = f\"{DRIVE_ROOT}/data/samples/all_val_ids.txt\"\n\nprint(\"Paths configured:\")\nprint(f\"  SEVIR_ROOT: {SEVIR_ROOT}\")\nprint(f\"  Catalog exists: {os.path.exists(CATALOG_PATH)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from stormfusion.data.sevir_multimodal import SEVIRMultiModalDataset\n\nprint(\"✅ Dataset class imported\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Index (Test with Small Subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\nprint(\"Loading catalog and building event list...\\n\")\n\n# Load catalog\ncatalog = pd.read_csv(CATALOG_PATH, low_memory=False)\nprint(f\"Total catalog entries: {len(catalog)}\")\n\n# Filter to only VIL events (target modality)\nvil_catalog = catalog[catalog['img_type'] == 'vil']\nprint(f\"VIL entries: {len(vil_catalog)}\")\n\n# Filter to only 2019 (the data we have)\nvil_catalog_2019 = vil_catalog[vil_catalog['file_name'].str.contains('2019')]\nprint(f\"VIL entries from 2019: {len(vil_catalog_2019)}\")\n\n# Get unique event IDs\nall_event_ids = vil_catalog_2019['id'].unique()\nprint(f\"Unique events: {len(all_event_ids)}\")\n\n# Create train/val split (80/20)\nimport numpy as np\nnp.random.seed(42)\nindices = np.random.permutation(len(all_event_ids))\nsplit_idx = int(0.8 * len(all_event_ids))\n\ntrain_event_ids = all_event_ids[indices[:split_idx]]\nval_event_ids = all_event_ids[indices[split_idx:]]\n\nprint(f\"\\nTrain events: {len(train_event_ids)}\")\nprint(f\"Val events: {len(val_event_ids)}\")\n\n# For testing, use small subset\ntest_train_ids = train_event_ids[:10]\ntest_val_ids = val_event_ids[:5]\n\nprint(f\"\\nTest subset:\")\nprint(f\"  Train: {len(test_train_ids)} events\")\nprint(f\"  Val: {len(test_val_ids)} events\")\nprint(f\"\\n✅ Event IDs prepared\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Dataset (Small Subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Building dataset index from event IDs...\\n\")\n\n# Build index: List of (event_id, file_index) tuples\ndef build_index_from_ids(catalog, event_ids, modality='vil'):\n    \"\"\"Build index from event ID list.\"\"\"\n    modality_cat = catalog[catalog[\"img_type\"] == modality].copy()\n    \n    index = []\n    for event_id in event_ids:\n        event_rows = modality_cat[modality_cat[\"id\"] == event_id]\n        if not event_rows.empty:\n            row = event_rows.iloc[0]\n            index.append((event_id, int(row[\"file_index\"])))\n    \n    return index\n\n# Build indices for our test subset\ntrain_index = build_index_from_ids(catalog, test_train_ids)\nprint(f\"Train index: {len(train_index)} events\")\n\n# Create dataset\nfrom stormfusion.data.sevir_multimodal import SEVIRMultiModalDataset\n\ntest_dataset = SEVIRMultiModalDataset(\n    index=train_index,  # List of (event_id, file_index) tuples\n    sevir_root=SEVIR_ROOT,\n    catalog_path=CATALOG_PATH,\n    input_steps=12,\n    output_steps=12,\n    normalize=True,\n    augment=False  # No augmentation for testing\n)\n\nprint(f\"✅ Dataset created with {len(test_dataset)} samples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load and Inspect Single Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Loading sample 0...\\n\")\n\ninputs, outputs = test_dataset[0]\n\nprint(\"INPUT SHAPES:\")\nfor modality, data in inputs.items():\n    shape_str = str(tuple(data.shape))\n    print(f\"  {modality:8s}: {shape_str} (T, H, W)\")\n    print(f\"             Range: [{data.min():.3f}, {data.max():.3f}]\")\n    print(f\"             Mean: {data.mean():.3f}, Std: {data.std():.3f}\")\n    \n    # Check for zeros (indicates missing data)\n    if data.abs().sum() < 0.01:\n        print(f\"             ⚠️  WARNING: All zeros (missing modality)\")\n    print()\n\nprint(\"OUTPUT SHAPE:\")\noutput_data = outputs['vil']  # Outputs is dict with 'vil' key\nshape_str = str(tuple(output_data.shape))\nprint(f\"  vil:      {shape_str} (T, H, W)\")\nprint(f\"            Range: [{output_data.min():.3f}, {output_data.max():.3f}]\")\n\nprint(\"\\n✅ Sample loaded successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\n# Show last input frame for each modality\nfor i, modality in enumerate(['vil', 'ir069', 'ir107', 'lght']):\n    data = inputs[modality][-1].numpy()  # Last timestep\n    im = axes[0, i].imshow(data, cmap='viridis', vmin=data.min(), vmax=data.max())\n    axes[0, i].set_title(f'{modality.upper()}\\n(input t=12)')\n    axes[0, i].axis('off')\n    plt.colorbar(im, ax=axes[0, i], fraction=0.046)\n\n# Show VIL predictions (first 4 output timesteps)\nvil_output = outputs['vil']  # Outputs is dict with 'vil' key\nfor i in range(4):\n    if i < vil_output.shape[0]:\n        data = vil_output[i].numpy()\n        im = axes[1, i].imshow(data, cmap='viridis', vmin=0, vmax=1)\n        axes[1, i].set_title(f'VIL Target\\n(t+{(i+1)*5}min)')\n        axes[1, i].axis('off')\n        plt.colorbar(im, ax=axes[1, i], fraction=0.046)\n\nplt.tight_layout()\nplt.savefig('/content/sample_visualization.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"✓ Saved to /content/sample_visualization.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Multiple Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading multiple samples...\\n\")\n",
    "\n",
    "num_samples = min(5, len(test_dataset))\n",
    "\n",
    "for idx in range(num_samples):\n",
    "    try:\n",
    "        inputs, outputs = test_dataset[idx]\n",
    "        print(f\"Sample {idx}: ✅ shapes correct\")\n",
    "    except Exception as e:\n",
    "        print(f\"Sample {idx}: ❌ Error: {e}\")\n",
    "\n",
    "print(f\"\\n✅ Successfully loaded {num_samples} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test DataLoader with Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from torch.utils.data import DataLoader\nfrom stormfusion.data.sevir_multimodal import multimodal_collate_fn\n\nprint(\"Creating DataLoader...\\n\")\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=2,\n    shuffle=False,\n    num_workers=0,  # Use 0 for Colab\n    collate_fn=multimodal_collate_fn  # Use custom collate function\n)\n\nprint(f\"DataLoader created: {len(test_loader)} batches\\n\")\n\n# Test loading one batch\nprint(\"Loading first batch...\")\ninputs_batch, outputs_batch = next(iter(test_loader))\n\nprint(\"\\nBatch shapes:\")\nprint(\"  Inputs:\")\nfor modality, data in inputs_batch.items():\n    shape_str = str(tuple(data.shape))\n    print(f\"    {modality:8s}: {shape_str} (B, T, H, W)\")\n\nprint(\"  Outputs:\")\nvil_batch = outputs_batch['vil']  # Outputs is dict with 'vil' key\nshape_str = str(tuple(vil_batch.shape))\nprint(f\"    vil:      {shape_str} (B, T, H, W)\")\n\nprint(\"\\n✅ Batching works correctly\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Testing augmentation...\\n\")\n\n# Build index for augmentation test\naug_index = build_index_from_ids(catalog, test_train_ids[:5])\n\n# Create dataset with augmentation  \naug_dataset = SEVIRMultiModalDataset(\n    index=aug_index,\n    sevir_root=SEVIR_ROOT,\n    catalog_path=CATALOG_PATH,\n    input_steps=12,\n    output_steps=12,\n    normalize=True,\n    augment=True  # Enable augmentation\n)\n\n# Load same sample multiple times to see augmentation\nprint(\"Loading same sample 3 times with augmentation:\\n\")\nfor i in range(3):\n    inputs, outputs = aug_dataset[0]\n    vil_sum = inputs['vil'].sum().item()\n    print(f\"  Iteration {i+1}: VIL sum = {vil_sum:.2f} (should vary due to flips/rotations)\")\n\nprint(\"\\n✅ Augmentation working (values change between iterations)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**What we tested:**\n",
    "- ✅ Dataset index building\n",
    "- ✅ Single sample loading\n",
    "- ✅ Data shapes are correct\n",
    "- ✅ Normalization applied\n",
    "- ✅ Batching works\n",
    "- ✅ Augmentation works\n",
    "\n",
    "**Warnings to check:**\n",
    "- If you see \"All zeros\" warnings, those modalities are missing from your data\n",
    "- Model will still run but use zeros for those channels\n",
    "- Download complete data for best performance\n",
    "\n",
    "**Next notebook:** `04_Test_Model_Components.ipynb`  \n",
    "This will test each model module individually before full integration."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}