{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 06: Small-Scale Training Test\n",
    "\n",
    "**Purpose:** Test the training loop with a tiny dataset (10-20 events)\n",
    "\n",
    "**What this does:**\n",
    "- Train SGT model on small subset\n",
    "- Verify training loop works\n",
    "- Check for overfitting (model should memorize small dataset)\n",
    "- Test validation and checkpointing\n",
    "- Measure training speed\n",
    "\n",
    "**What this does NOT do:**\n",
    "- Train on full dataset (that's notebook 07)\n",
    "- Run for many epochs\n",
    "- Compute final metrics\n",
    "\n",
    "**Expected time:** 10-20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:** \n",
    "- Run `01_Setup_and_Environment.ipynb` first\n",
    "- Run `02_Data_Verification.ipynb` to check/download data\n",
    "- Run `05_Test_Full_Model.ipynb` to verify model works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\nimport sys\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n\n# Mount Drive\nprint(\"Mounting Google Drive...\")\ndrive.mount('/content/drive', force_remount=False)\nprint(\"✅ Drive mounted\\n\")\n\n# Clone/update repository\nREPO_PATH = '/content/stormfusion-sevir'\nif not os.path.exists(REPO_PATH):\n    print(\"Cloning repository...\")\n    !git clone https://github.com/syedhaliz/stormfusion-sevir.git {REPO_PATH}\n    print(\"✅ Repository cloned\\n\")\nelse:\n    print(\"Repository exists, pulling latest changes...\")\n    !cd {REPO_PATH} && git pull\n    print(\"✅ Repository updated\\n\")\n\n# Add repository to path\nif REPO_PATH not in sys.path:\n    sys.path.insert(0, REPO_PATH)\n    print(f\"✅ Added {REPO_PATH} to Python path\\n\")\n\n# Force reload of modules to get latest code\nimport importlib\nfor module_name in ['stormfusion.models.sgt', 'stormfusion.data.sevir_multimodal']:\n    if module_name in sys.modules:\n        importlib.reload(sys.modules[module_name])\n        \nprint(\"✅ Modules reloaded\\n\")\n\n# Paths\nDRIVE_ROOT = \"/content/drive/MyDrive/SEVIR_Data\"\nSEVIR_ROOT = f\"{DRIVE_ROOT}/data/sevir\"\nCATALOG_PATH = f\"{DRIVE_ROOT}/data/SEVIR_CATALOG.csv\"\nCHECKPOINT_DIR = f\"{DRIVE_ROOT}/checkpoints/small_scale_test\"\n\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\\n\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: Load Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stormfusion.data.sevir_multimodal import SEVIRMultiModalDataset\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING SMALL DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load catalog\n",
    "catalog = pd.read_csv(CATALOG_PATH, low_memory=False)\n",
    "print(f\"\\nCatalog loaded: {len(catalog)} entries\")\n",
    "\n",
    "# Get small subset: 16 train, 4 val\n",
    "train_events = catalog[catalog['split'] == 'train']['id'].unique()[:16]\n",
    "val_events = catalog[catalog['split'] == 'val']['id'].unique()[:4]\n",
    "\n",
    "print(f\"\\nSmall dataset:\")\n",
    "print(f\"  Train events: {len(train_events)}\")\n",
    "print(f\"  Val events: {len(val_events)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SEVIRMultiModalDataset(\n",
    "    catalog=catalog,\n",
    "    data_root=SEVIR_ROOT,\n",
    "    event_ids=train_events,\n",
    "    input_steps=12,\n",
    "    output_steps=12,\n",
    "    modalities=['vil', 'ir069', 'ir107', 'lght'],\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "val_dataset = SEVIRMultiModalDataset(\n",
    "    catalog=catalog,\n",
    "    data_root=SEVIR_ROOT,\n",
    "    event_ids=val_events,\n",
    "    input_steps=12,\n",
    "    output_steps=12,\n",
    "    modalities=['vil', 'ir069', 'ir107', 'lght'],\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val: {len(val_dataset)} samples\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataloader batches:\")\n",
    "print(f\"  Train: {len(train_loader)} batches\")\n",
    "print(f\"  Val: {len(val_loader)} batches\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 3: Create Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stormfusion.models.sgt import create_sgt_model\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create model\n",
    "model = create_sgt_model(\n",
    "    modalities=['vil', 'ir069', 'ir107', 'lght'],\n",
    "    input_steps=12,\n",
    "    output_steps=12,\n",
    "    base_channels=64\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Loss criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"\\n✅ Model, optimizer, and scheduler created\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 4: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STARTING SMALL-SCALE TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "num_epochs = 5\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': [], 'lr': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # ============= TRAINING =============\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Add physics loss if available\n",
    "        if hasattr(model, 'physics_loss'):\n",
    "            physics_loss = model.physics_loss(outputs, targets)\n",
    "            loss = loss + physics_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # ============= VALIDATION =============\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader, desc='Validation'):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            if hasattr(model, 'physics_loss'):\n",
    "                physics_loss = model.physics_loss(outputs, targets)\n",
    "                loss = loss + physics_loss\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(avg_val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.6f}\")\n",
    "    print(f\"  Val Loss:   {avg_val_loss:.6f}\")\n",
    "    print(f\"  LR:         {current_lr:.2e}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"{CHECKPOINT_DIR}/best_model.pt\")\n",
    "        print(f\"  ✅ Saved best model (val_loss: {best_val_loss:.6f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBest validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 5: Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1].plot(history['lr'], marker='o', color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CHECKPOINT_DIR}/training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Training curves saved to:\", f\"{CHECKPOINT_DIR}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 6: Test Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTING MODEL PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(f\"{CHECKPOINT_DIR}/best_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nLoaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"Validation loss: {checkpoint['val_loss']:.6f}\")\n",
    "\n",
    "# Get a validation sample\n",
    "inputs, targets = val_dataset[0]\n",
    "\n",
    "# Add batch dimension\n",
    "inputs_batch = {k: v.unsqueeze(0).to(device) for k, v in inputs.items()}\n",
    "targets_batch = targets.unsqueeze(0).to(device)\n",
    "\n",
    "# Generate prediction\n",
    "with torch.no_grad():\n",
    "    predictions = model(inputs_batch)\n",
    "\n",
    "# Move to CPU for visualization\n",
    "predictions = predictions.cpu().squeeze(0).numpy()\n",
    "targets = targets.numpy()\n",
    "\n",
    "print(f\"\\nPrediction shape: {predictions.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")\n",
    "\n",
    "# Visualize middle frame (frame 6)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "frame_idx = 6\n",
    "\n",
    "# Input (VIL)\n",
    "vil_input = inputs['vil'][frame_idx].numpy()\n",
    "im0 = axes[0].imshow(vil_input, cmap='turbo', vmin=0, vmax=255)\n",
    "axes[0].set_title(f'Input VIL (t={frame_idx})')\n",
    "axes[0].axis('off')\n",
    "plt.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "# Prediction\n",
    "im1 = axes[1].imshow(predictions[frame_idx], cmap='turbo', vmin=0, vmax=255)\n",
    "axes[1].set_title(f'Prediction (t={frame_idx+12})')\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "# Ground truth\n",
    "im2 = axes[2].imshow(targets[frame_idx], cmap='turbo', vmin=0, vmax=255)\n",
    "axes[2].set_title(f'Ground Truth (t={frame_idx+12})')\n",
    "axes[2].axis('off')\n",
    "plt.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CHECKPOINT_DIR}/sample_prediction.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Sample prediction saved to:\", f\"{CHECKPOINT_DIR}/sample_prediction.png\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we verified:**\n",
    "- ✅ Training loop works end-to-end\n",
    "- ✅ Model can learn from small dataset\n",
    "- ✅ Validation and checkpointing work\n",
    "- ✅ Loss decreases over epochs\n",
    "- ✅ Model generates reasonable predictions\n",
    "\n",
    "**Training configuration:**\n",
    "```\n",
    "Dataset: 16 train events, 4 val events\n",
    "Batch size: 2\n",
    "Epochs: 5\n",
    "Optimizer: AdamW (lr=1e-4)\n",
    "Scheduler: ReduceLROnPlateau\n",
    "```\n",
    "\n",
    "**Expected behavior:**\n",
    "- Training loss should decrease consistently\n",
    "- Validation loss should decrease (model memorizes small dataset)\n",
    "- If validation loss increases, dataset might have missing modalities\n",
    "\n",
    "**Next steps:**\n",
    "1. If training worked ✅, proceed to `07_Full_Training.ipynb`\n",
    "2. That notebook will train on the full dataset\n",
    "3. You'll need complete SEVIR data (from notebook 02)\n",
    "4. Full training takes several hours to days depending on dataset size\n",
    "\n",
    "---\n",
    "\n",
    "**Troubleshooting:**\n",
    "- If loss doesn't decrease: Check for missing modalities (all zeros)\n",
    "- If OOM error: Reduce batch size or use gradient accumulation\n",
    "- If NaN loss: Add gradient clipping (already included)\n",
    "- If slow training: Check GPU utilization with `nvidia-smi`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}