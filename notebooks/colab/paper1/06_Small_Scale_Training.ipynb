{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 06: Small-Scale Training Test\n",
    "\n",
    "**Purpose:** Test the training loop with a tiny dataset (10-20 events)\n",
    "\n",
    "**What this does:**\n",
    "- Train SGT model on small subset\n",
    "- Verify training loop works\n",
    "- Check for overfitting (model should memorize small dataset)\n",
    "- Test validation and checkpointing\n",
    "- Measure training speed\n",
    "\n",
    "**What this does NOT do:**\n",
    "- Train on full dataset (that's notebook 07)\n",
    "- Run for many epochs\n",
    "- Compute final metrics\n",
    "\n",
    "**Expected time:** 10-20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:** \n",
    "- Run `01_Setup_and_Environment.ipynb` first\n",
    "- Run `02_Data_Verification.ipynb` to check/download data\n",
    "- Run `05_Test_Full_Model.ipynb` to verify model works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\nimport sys\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n\n# Mount Drive\nprint(\"Mounting Google Drive...\")\ndrive.mount('/content/drive', force_remount=False)\nprint(\"✅ Drive mounted\\n\")\n\n# Install dependencies (each Colab session needs this!)\nprint(\"Installing dependencies...\")\n!pip install -q torch-geometric h5py pandas tqdm matplotlib lpips scikit-image scipy\nprint(\"✅ Dependencies installed\\n\")\n\n# Clone/update repository\nREPO_PATH = '/content/stormfusion-sevir'\nif not os.path.exists(REPO_PATH):\n    print(\"Cloning repository...\")\n    !git clone https://github.com/syedhaliz/stormfusion-sevir.git {REPO_PATH}\n    print(\"✅ Repository cloned\\n\")\nelse:\n    print(\"Repository exists, pulling latest changes...\")\n    !cd {REPO_PATH} && git pull\n    print(\"✅ Repository updated\\n\")\n\n# Add repository to path\nif REPO_PATH not in sys.path:\n    sys.path.insert(0, REPO_PATH)\n    print(f\"✅ Added {REPO_PATH} to Python path\\n\")\n\n# Force reload of modules to get latest code\nimport importlib\nfor module_name in ['stormfusion.models.sgt', 'stormfusion.data.sevir_multimodal']:\n    if module_name in sys.modules:\n        importlib.reload(sys.modules[module_name])\n        \nprint(\"✅ Modules reloaded\\n\")\n\n# Paths\nDRIVE_ROOT = \"/content/drive/MyDrive/SEVIR_Data\"\nSEVIR_ROOT = f\"{DRIVE_ROOT}/data/sevir\"\nCATALOG_PATH = f\"{DRIVE_ROOT}/data/SEVIR_CATALOG.csv\"\nCHECKPOINT_DIR = f\"{DRIVE_ROOT}/checkpoints/small_scale_test\"\n\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\\n\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: Load Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "from stormfusion.data.sevir_multimodal import SEVIRMultiModalDataset\n\nprint(\"=\"*70)\nprint(\"LOADING SMALL DATASET\")\nprint(\"=\"*70)\n\n# Load catalog\ncatalog = pd.read_csv(CATALOG_PATH, low_memory=False)\nprint(f\"\\nCatalog loaded: {len(catalog)} entries\")\n\n# Get small subset of event IDs (first 20 events: 16 train, 4 val)\nall_event_ids = catalog['id'].unique()\ntrain_event_ids = all_event_ids[:16]\nval_event_ids = all_event_ids[16:20]\n\nprint(f\"\\nSmall dataset:\")\nprint(f\"  Train events: {len(train_event_ids)}\")\nprint(f\"  Val events: {len(val_event_ids)}\")\n\n# Build indices for train and val\nvil_catalog = catalog[catalog['img_type'] == 'vil']\n\ntrain_index = []\nfor event_id in train_event_ids:\n    event_rows = vil_catalog[vil_catalog['id'] == event_id]\n    if not event_rows.empty:\n        row = event_rows.iloc[0]\n        train_index.append((event_id, int(row['file_index'])))\n\nval_index = []\nfor event_id in val_event_ids:\n    event_rows = vil_catalog[vil_catalog['id'] == event_id]\n    if not event_rows.empty:\n        row = event_rows.iloc[0]\n        val_index.append((event_id, int(row['file_index'])))\n\nprint(f\"\\nBuilt indices:\")\nprint(f\"  Train: {len(train_index)} samples\")\nprint(f\"  Val: {len(val_index)} samples\")\n\n# Create datasets\ntrain_dataset = SEVIRMultiModalDataset(\n    index=train_index,\n    sevir_root=SEVIR_ROOT,\n    catalog_path=CATALOG_PATH,\n    input_steps=12,\n    output_steps=12,\n    normalize=True,\n    augment=True\n)\n\nval_dataset = SEVIRMultiModalDataset(\n    index=val_index,\n    sevir_root=SEVIR_ROOT,\n    catalog_path=CATALOG_PATH,\n    input_steps=12,\n    output_steps=12,\n    normalize=True,\n    augment=False\n)\n\nprint(f\"\\nDataset sizes:\")\nprint(f\"  Train: {len(train_dataset)} samples\")\nprint(f\"  Val: {len(val_dataset)} samples\")\n\n# Create dataloaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=2,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=2,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\"\\nDataloader batches:\")\nprint(f\"  Train: {len(train_loader)} batches\")\nprint(f\"  Val: {len(val_loader)} batches\")\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 3: Create Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "from stormfusion.models.sgt import create_sgt_model\n\nprint(\"=\"*70)\nprint(\"CREATING MODEL\")\nprint(\"=\"*70)\n\n# Create model with config dict\nconfig = {\n    'modalities': ['vil', 'ir069', 'ir107', 'lght'],\n    'input_steps': 12,\n    'output_steps': 12,\n    'hidden_dim': 128,\n    'gnn_layers': 3,\n    'transformer_layers': 4,\n    'num_heads': 8,\n    'use_physics': True\n}\n\nmodel = create_sgt_model(config).to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\nTotal parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n\n# Create optimizer\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=1e-4,\n    weight_decay=1e-5\n)\n\n# Learning rate scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.5,\n    patience=2,\n    verbose=True\n)\n\nprint(\"\\n✅ Model, optimizer, and scheduler created\")\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 4: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"STARTING SMALL-SCALE TRAINING\")\nprint(\"=\"*70)\n\nnum_epochs = 5\nbest_val_loss = float('inf')\nhistory = {'train_loss': [], 'val_loss': [], 'lr': []}\n\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n    print(\"-\" * 50)\n    \n    # ============= TRAINING =============\n    model.train()\n    train_loss = 0.0\n    \n    pbar = tqdm(train_loader, desc='Training')\n    for batch_idx, (inputs, outputs_dict) in enumerate(pbar):\n        # Move to device\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        # Dataset returns outputs dict with 'vil' key\n        targets = outputs_dict['vil'].to(device)\n        \n        # Forward pass (model returns tuple!)\n        optimizer.zero_grad()\n        predictions, attention_info, physics_info = model(inputs)\n        \n        # Compute loss using model's method\n        loss, loss_dict = model.compute_loss(\n            predictions=predictions,\n            targets=targets,\n            physics_info=physics_info,\n            lambda_mse=1.0,\n            lambda_physics=0.1,\n            lambda_extreme=2.0\n        )\n        \n        # Backward pass\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        # Update metrics\n        train_loss += loss.item()\n        pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n    \n    avg_train_loss = train_loss / len(train_loader)\n    \n    # ============= VALIDATION =============\n    model.eval()\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for inputs, outputs_dict in tqdm(val_loader, desc='Validation'):\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            targets = outputs_dict['vil'].to(device)\n            \n            # Forward pass (unpack tuple)\n            predictions, attention_info, physics_info = model(inputs)\n            \n            # Compute loss\n            loss, loss_dict = model.compute_loss(\n                predictions=predictions,\n                targets=targets,\n                physics_info=physics_info,\n                lambda_mse=1.0,\n                lambda_physics=0.1,\n                lambda_extreme=2.0\n            )\n            \n            val_loss += loss.item()\n    \n    avg_val_loss = val_loss / len(val_loader)\n    \n    # Update learning rate\n    scheduler.step(avg_val_loss)\n    current_lr = optimizer.param_groups[0]['lr']\n    \n    # Save history\n    history['train_loss'].append(avg_train_loss)\n    history['val_loss'].append(avg_val_loss)\n    history['lr'].append(current_lr)\n    \n    # Print epoch summary\n    print(f\"\\nEpoch {epoch+1} Summary:\")\n    print(f\"  Train Loss: {avg_train_loss:.6f}\")\n    print(f\"  Val Loss:   {avg_val_loss:.6f}\")\n    print(f\"  LR:         {current_lr:.2e}\")\n    \n    # Save best model\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'train_loss': avg_train_loss,\n            'val_loss': avg_val_loss,\n        }\n        torch.save(checkpoint, f\"{CHECKPOINT_DIR}/best_model.pt\")\n        print(f\"  ✅ Saved best model (val_loss: {best_val_loss:.6f})\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*70)\nprint(f\"\\nBest validation loss: {best_val_loss:.6f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 5: Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1].plot(history['lr'], marker='o', color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CHECKPOINT_DIR}/training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Training curves saved to:\", f\"{CHECKPOINT_DIR}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 6: Test Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"TESTING MODEL PREDICTIONS\")\nprint(\"=\"*70)\n\n# Load best model\ncheckpoint = torch.load(f\"{CHECKPOINT_DIR}/best_model.pt\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\nprint(f\"\\nLoaded best model from epoch {checkpoint['epoch']+1}\")\nprint(f\"Validation loss: {checkpoint['val_loss']:.6f}\")\n\n# Get a validation sample\ninputs, outputs_dict = val_dataset[0]\n\n# Extract VIL target from outputs dict\ntargets = outputs_dict['vil']\n\n# Add batch dimension\ninputs_batch = {k: v.unsqueeze(0).to(device) for k, v in inputs.items()}\n\n# Generate prediction (unpack tuple!)\nwith torch.no_grad():\n    predictions, _, _ = model(inputs_batch)\n\n# Move to CPU for visualization\npredictions = predictions.cpu().squeeze(0).numpy()\ntargets = targets.numpy()\n\nprint(f\"\\nPrediction shape: {predictions.shape}\")\nprint(f\"Target shape: {targets.shape}\")\n\n# Visualize middle frame (frame 6)\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\nframe_idx = 6\n\n# Input (VIL) - last input frame\nvil_input = inputs['vil'][-1].numpy()  # Use last input frame\nim0 = axes[0].imshow(vil_input, cmap='turbo', vmin=0, vmax=1)\naxes[0].set_title(f'Input VIL (t=11, last input)')\naxes[0].axis('off')\nplt.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n\n# Prediction - middle output frame\nim1 = axes[1].imshow(predictions[frame_idx], cmap='turbo', vmin=0, vmax=1)\naxes[1].set_title(f'Prediction (t={12+frame_idx})')\naxes[1].axis('off')\nplt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n\n# Ground truth - middle output frame\nim2 = axes[2].imshow(targets[frame_idx], cmap='turbo', vmin=0, vmax=1)\naxes[2].set_title(f'Ground Truth (t={12+frame_idx})')\naxes[2].axis('off')\nplt.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.savefig(f\"{CHECKPOINT_DIR}/sample_prediction.png\", dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n✅ Sample prediction saved to:\", f\"{CHECKPOINT_DIR}/sample_prediction.png\")\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we verified:**\n",
    "- ✅ Training loop works end-to-end\n",
    "- ✅ Model can learn from small dataset\n",
    "- ✅ Validation and checkpointing work\n",
    "- ✅ Loss decreases over epochs\n",
    "- ✅ Model generates reasonable predictions\n",
    "\n",
    "**Training configuration:**\n",
    "```\n",
    "Dataset: 16 train events, 4 val events\n",
    "Batch size: 2\n",
    "Epochs: 5\n",
    "Optimizer: AdamW (lr=1e-4)\n",
    "Scheduler: ReduceLROnPlateau\n",
    "```\n",
    "\n",
    "**Expected behavior:**\n",
    "- Training loss should decrease consistently\n",
    "- Validation loss should decrease (model memorizes small dataset)\n",
    "- If validation loss increases, dataset might have missing modalities\n",
    "\n",
    "**Next steps:**\n",
    "1. If training worked ✅, proceed to `07_Full_Training.ipynb`\n",
    "2. That notebook will train on the full dataset\n",
    "3. You'll need complete SEVIR data (from notebook 02)\n",
    "4. Full training takes several hours to days depending on dataset size\n",
    "\n",
    "---\n",
    "\n",
    "**Troubleshooting:**\n",
    "- If loss doesn't decrease: Check for missing modalities (all zeros)\n",
    "- If OOM error: Reduce batch size or use gradient accumulation\n",
    "- If NaN loss: Add gradient clipping (already included)\n",
    "- If slow training: Check GPU utilization with `nvidia-smi`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}