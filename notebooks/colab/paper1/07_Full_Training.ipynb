{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 07: Full-Scale Training (Paper 1 - Storm-Graph Transformer)\n",
    "\n",
    "**Purpose:** Train SGT model on full SEVIR dataset for Paper 1\n",
    "\n",
    "**What this does:**\n",
    "- Train on full dataset (hundreds of events)\n",
    "- Run for many epochs (20-50)\n",
    "- Track metrics: MSE, MAE, CSI\n",
    "- Save checkpoints regularly\n",
    "- Generate evaluation visualizations\n",
    "- Compare with baseline methods\n",
    "\n",
    "**What this does NOT do:**\n",
    "- Use Stage 5 techniques (that's Paper 2)\n",
    "- Run ablation studies (create separate notebook if needed)\n",
    "\n",
    "**Expected time:** Several hours to days (depending on dataset size)\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:** \n",
    "- Run `01_Setup_and_Environment.ipynb` first\n",
    "- Run `02_Data_Verification.ipynb` and download FULL dataset\n",
    "- Run `06_Small_Scale_Training.ipynb` to verify training works\n",
    "- Ensure you have sufficient GPU memory and disk space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\nimport sys\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport json\nfrom datetime import datetime\n\n# Mount Drive\nprint(\"Mounting Google Drive...\")\ndrive.mount('/content/drive', force_remount=False)\nprint(\"✅ Drive mounted\\n\")\n\n# Install dependencies (each Colab session needs this!)\nprint(\"Installing dependencies...\")\n!pip install -q torch-geometric h5py pandas tqdm matplotlib lpips scikit-image scipy\nprint(\"✅ Dependencies installed\\n\")\n\n# Clone/update repository\nREPO_PATH = '/content/stormfusion-sevir'\nif not os.path.exists(REPO_PATH):\n    print(\"Cloning repository...\")\n    !git clone https://github.com/syedhaliz/stormfusion-sevir.git {REPO_PATH}\n    print(\"✅ Repository cloned\\n\")\nelse:\n    print(\"Repository exists, pulling latest changes...\")\n    !cd {REPO_PATH} && git pull\n    print(\"✅ Repository updated\\n\")\n\n# Add repository to path\nif REPO_PATH not in sys.path:\n    sys.path.insert(0, REPO_PATH)\n    print(f\"✅ Added {REPO_PATH} to Python path\\n\")\n\n# Force reload of modules to get latest code\nimport importlib\nfor module_name in ['stormfusion.models.sgt', 'stormfusion.data.sevir_multimodal']:\n    if module_name in sys.modules:\n        importlib.reload(sys.modules[module_name])\n        \nprint(\"✅ Modules reloaded\\n\")\n\n# Paths\nDRIVE_ROOT = \"/content/drive/MyDrive/SEVIR_Data\"\nSEVIR_ROOT = f\"{DRIVE_ROOT}/data/sevir\"\nCATALOG_PATH = f\"{DRIVE_ROOT}/data/SEVIR_CATALOG.csv\"\n\n# Create experiment directory with timestamp\nEXPERIMENT_NAME = f\"sgt_full_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\nCHECKPOINT_DIR = f\"{DRIVE_ROOT}/experiments/{EXPERIMENT_NAME}\"\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\nprint(f\"Experiment: {EXPERIMENT_NAME}\")\nprint(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\nprint(f\"\\nPyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\\n\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Training configuration\nconfig = {\n    # Model\n    'modalities': ['vil', 'ir069', 'ir107', 'lght'],\n    'input_steps': 12,\n    'output_steps': 12,\n    'hidden_dim': 128,  # ← Fixed: was base_channels\n    'gnn_layers': 3,\n    'transformer_layers': 4,\n    'num_heads': 8,\n    'use_physics': True,\n    \n    # Training\n    'num_epochs': 30,\n    'batch_size': 4,\n    'num_workers': 4,\n    'learning_rate': 1e-4,\n    'weight_decay': 1e-5,\n    'grad_clip': 1.0,\n    \n    # Loss weights\n    'lambda_mse': 1.0,\n    'lambda_physics': 0.1,\n    'lambda_extreme': 2.0,\n    \n    # Scheduler\n    'scheduler_patience': 3,\n    'scheduler_factor': 0.5,\n    'min_lr': 1e-7,\n    \n    # Data\n    'train_augment': True,\n    'max_train_events': None,  # None = use all\n    'max_val_events': 100,     # Limit validation for speed\n    \n    # Checkpointing\n    'save_every': 5,  # Save checkpoint every N epochs\n    'keep_last_n': 3,  # Keep only last N checkpoints\n}\n\n# Save config\nwith open(f\"{CHECKPOINT_DIR}/config.json\", 'w') as f:\n    json.dump(config, f, indent=2)\n\nprint(\"Training configuration:\")\nprint(json.dumps(config, indent=2))"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 3: Load Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "from stormfusion.data.sevir_multimodal import SEVIRMultiModalDataset\n\nprint(\"=\"*70)\nprint(\"LOADING FULL DATASET\")\nprint(\"=\"*70)\n\n# Load catalog\ncatalog = pd.read_csv(CATALOG_PATH, low_memory=False)\nprint(f\"\\nCatalog loaded: {len(catalog)} entries\")\n\n# Get all unique events\nall_event_ids = catalog['id'].unique()\nprint(f\"Total unique events: {len(all_event_ids)}\")\n\n# Split: 80% train, 20% val\nn_events = len(all_event_ids)\nn_train = int(0.8 * n_events)\n\nall_train_events = all_event_ids[:n_train]\nall_val_events = all_event_ids[n_train:]\n\nprint(f\"\\nDataset split:\")\nprint(f\"  Train events: {len(all_train_events)}\")\nprint(f\"  Val events: {len(all_val_events)}\")\n\n# Apply limits if specified\nif config['max_train_events'] is not None:\n    train_events = all_train_events[:config['max_train_events']]\n    print(f\"  Using train events: {len(train_events)} (limited)\")\nelse:\n    train_events = all_train_events\n    print(f\"  Using train events: {len(train_events)} (all)\")\n\nif config['max_val_events'] is not None:\n    val_events = all_val_events[:config['max_val_events']]\n    print(f\"  Using val events: {len(val_events)} (limited)\")\nelse:\n    val_events = all_val_events\n    print(f\"  Using val events: {len(val_events)} (all)\")\n\n# Build indices (event_id, file_index) tuples\nprint(\"\\nBuilding dataset indices...\")\nvil_catalog = catalog[catalog['img_type'] == 'vil']\n\ntrain_index = []\nfor event_id in train_events:\n    event_rows = vil_catalog[vil_catalog['id'] == event_id]\n    if not event_rows.empty:\n        row = event_rows.iloc[0]\n        train_index.append((event_id, int(row['file_index'])))\n\nval_index = []\nfor event_id in val_events:\n    event_rows = vil_catalog[vil_catalog['id'] == event_id]\n    if not event_rows.empty:\n        row = event_rows.iloc[0]\n        val_index.append((event_id, int(row['file_index'])))\n\nprint(f\"Indices built: {len(train_index)} train, {len(val_index)} val\")\n\n# Create datasets\nprint(\"\\nCreating datasets...\")\ntrain_dataset = SEVIRMultiModalDataset(\n    index=train_index,\n    sevir_root=SEVIR_ROOT,\n    catalog_path=CATALOG_PATH,\n    input_steps=config['input_steps'],\n    output_steps=config['output_steps'],\n    normalize=True,\n    augment=config['train_augment']\n)\n\nval_dataset = SEVIRMultiModalDataset(\n    index=val_index,\n    sevir_root=SEVIR_ROOT,\n    catalog_path=CATALOG_PATH,\n    input_steps=config['input_steps'],\n    output_steps=config['output_steps'],\n    normalize=True,\n    augment=False\n)\n\nprint(f\"\\nDataset sizes:\")\nprint(f\"  Train: {len(train_dataset)} samples\")\nprint(f\"  Val: {len(val_dataset)} samples\")\n\n# Create dataloaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=config['batch_size'],\n    shuffle=True,\n    num_workers=config['num_workers'],\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=config['batch_size'],\n    shuffle=False,\n    num_workers=config['num_workers'],\n    pin_memory=True\n)\n\nprint(f\"\\nDataloader info:\")\nprint(f\"  Train batches: {len(train_loader)}\")\nprint(f\"  Val batches: {len(val_loader)}\")\nprint(f\"  Batch size: {config['batch_size']}\")\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 4: Create Model and Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "from stormfusion.models.sgt import create_sgt_model\n\nprint(\"=\"*70)\nprint(\"CREATING MODEL AND TRAINING COMPONENTS\")\nprint(\"=\"*70)\n\n# Extract model config\nmodel_config = {\n    'modalities': config['modalities'],\n    'input_steps': config['input_steps'],\n    'output_steps': config['output_steps'],\n    'hidden_dim': config['hidden_dim'],\n    'gnn_layers': config['gnn_layers'],\n    'transformer_layers': config['transformer_layers'],\n    'num_heads': config['num_heads'],\n    'use_physics': config['use_physics']\n}\n\n# Create model\nmodel = create_sgt_model(model_config).to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\nModel created: {total_params:,} parameters ({total_params/1e6:.2f}M)\")\n\n# Create optimizer\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=config['learning_rate'],\n    weight_decay=config['weight_decay']\n)\n\n# Learning rate scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=config['scheduler_factor'],\n    patience=config['scheduler_patience'],\n    min_lr=config['min_lr'],\n    verbose=True\n)\n\nprint(\"\\n✅ All components created\")\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"STARTING FULL-SCALE TRAINING\")\nprint(\"=\"*70)\n\nbest_val_loss = float('inf')\nhistory = {\n    'train_loss': [],\n    'val_loss': [],\n    'train_mse': [],\n    'val_mse': [],\n    'lr': [],\n    'epoch_times': []\n}\n\nimport time\n\nfor epoch in range(config['num_epochs']):\n    epoch_start = time.time()\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n    print(f\"{'='*70}\")\n    \n    # ============= TRAINING =============\n    model.train()\n    train_loss = 0.0\n    train_mse = 0.0\n    \n    pbar = tqdm(train_loader, desc='Training')\n    for batch_idx, (inputs, outputs_dict) in enumerate(pbar):\n        # Move to device\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        # Dataset returns outputs dict with 'vil' key\n        targets = outputs_dict['vil'].to(device)\n        \n        # Forward pass (model returns tuple!)\n        optimizer.zero_grad()\n        predictions, attention_info, physics_info = model(inputs)\n        \n        # Compute loss using model's method\n        loss, loss_dict = model.compute_loss(\n            predictions=predictions,\n            targets=targets,\n            physics_info=physics_info,\n            lambda_mse=config['lambda_mse'],\n            lambda_physics=config['lambda_physics'],\n            lambda_extreme=config['lambda_extreme']\n        )\n        \n        # Backward pass\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['grad_clip'])\n        \n        optimizer.step()\n        \n        # Update metrics\n        train_loss += loss.item()\n        train_mse += loss_dict['mse']\n        \n        pbar.set_postfix({\n            'loss': f'{loss.item():.6f}',\n            'mse': f'{loss_dict[\"mse\"]:.6f}'\n        })\n    \n    avg_train_loss = train_loss / len(train_loader)\n    avg_train_mse = train_mse / len(train_loader)\n    \n    # ============= VALIDATION =============\n    model.eval()\n    val_loss = 0.0\n    val_mse = 0.0\n    \n    with torch.no_grad():\n        for inputs, outputs_dict in tqdm(val_loader, desc='Validation'):\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            targets = outputs_dict['vil'].to(device)\n            \n            # Forward pass (unpack tuple)\n            predictions, attention_info, physics_info = model(inputs)\n            \n            # Compute loss\n            loss, loss_dict = model.compute_loss(\n                predictions=predictions,\n                targets=targets,\n                physics_info=physics_info,\n                lambda_mse=config['lambda_mse'],\n                lambda_physics=config['lambda_physics'],\n                lambda_extreme=config['lambda_extreme']\n            )\n            \n            val_loss += loss.item()\n            val_mse += loss_dict['mse']\n    \n    avg_val_loss = val_loss / len(val_loader)\n    avg_val_mse = val_mse / len(val_loader)\n    \n    # Update learning rate\n    scheduler.step(avg_val_loss)\n    current_lr = optimizer.param_groups[0]['lr']\n    \n    # Compute epoch time\n    epoch_time = time.time() - epoch_start\n    \n    # Save history\n    history['train_loss'].append(avg_train_loss)\n    history['val_loss'].append(avg_val_loss)\n    history['train_mse'].append(avg_train_mse)\n    history['val_mse'].append(avg_val_mse)\n    history['lr'].append(current_lr)\n    history['epoch_times'].append(epoch_time)\n    \n    # Print epoch summary\n    print(f\"\\nEpoch {epoch+1} Summary:\")\n    print(f\"  Train Loss: {avg_train_loss:.6f}\")\n    print(f\"  Val Loss:   {avg_val_loss:.6f}\")\n    print(f\"  Train MSE:  {avg_train_mse:.6f}\")\n    print(f\"  Val MSE:    {avg_val_mse:.6f}\")\n    print(f\"  LR:         {current_lr:.2e}\")\n    print(f\"  Time:       {epoch_time/60:.1f} min\")\n    \n    # Save checkpoint\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'config': config,\n        'history': history,\n        'train_loss': avg_train_loss,\n        'val_loss': avg_val_loss,\n    }\n    \n    # Save best model\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        torch.save(checkpoint, f\"{CHECKPOINT_DIR}/best_model.pt\")\n        print(f\"  ✅ Saved best model (val_loss: {best_val_loss:.6f})\")\n    \n    # Save latest checkpoint\n    torch.save(checkpoint, f\"{CHECKPOINT_DIR}/latest_checkpoint.pt\")\n    \n    # Save periodic checkpoint\n    if (epoch + 1) % config['save_every'] == 0:\n        torch.save(checkpoint, f\"{CHECKPOINT_DIR}/checkpoint_epoch_{epoch+1}.pt\")\n        print(f\"  💾 Saved checkpoint at epoch {epoch+1}\")\n    \n    # Save history\n    with open(f\"{CHECKPOINT_DIR}/history.json\", 'w') as f:\n        json.dump(history, f, indent=2)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*70)\nprint(f\"\\nBest validation loss: {best_val_loss:.6f}\")\nprint(f\"Total training time: {sum(history['epoch_times'])/3600:.2f} hours\")\nprint(f\"\\nAll results saved to: {CHECKPOINT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 6: Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Loss curve\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o', markersize=4)\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s', markersize=4)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# MSE curve\n",
    "axes[0, 1].plot(history['train_mse'], label='Train MSE', marker='o', markersize=4)\n",
    "axes[0, 1].plot(history['val_mse'], label='Val MSE', marker='s', markersize=4)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('MSE')\n",
    "axes[0, 1].set_title('Mean Squared Error')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 0].plot(history['lr'], marker='o', color='green', markersize=4)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].set_title('Learning Rate Schedule')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Epoch time\n",
    "epoch_times_min = [t/60 for t in history['epoch_times']]\n",
    "axes[1, 1].plot(epoch_times_min, marker='o', color='purple', markersize=4)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Time (minutes)')\n",
    "axes[1, 1].set_title('Training Time per Epoch')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CHECKPOINT_DIR}/training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Training curves saved to: {CHECKPOINT_DIR}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"EVALUATING BEST MODEL\")\nprint(\"=\"*70)\n\n# Load best model\ncheckpoint = torch.load(f\"{CHECKPOINT_DIR}/best_model.pt\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\nprint(f\"\\nLoaded best model from epoch {checkpoint['epoch']+1}\")\nprint(f\"Validation loss: {checkpoint['val_loss']:.6f}\")\n\n# Evaluate on validation set\nprint(\"\\nComputing metrics on validation set...\")\n\nall_predictions = []\nall_targets = []\n\nwith torch.no_grad():\n    for inputs, outputs_dict in tqdm(val_loader, desc='Evaluating'):\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        targets = outputs_dict['vil'].to(device)\n        \n        # Forward pass (unpack tuple!)\n        predictions, _, _ = model(inputs)\n        \n        all_predictions.append(predictions.cpu())\n        all_targets.append(targets.cpu())\n\n# Concatenate\nall_predictions = torch.cat(all_predictions, dim=0)\nall_targets = torch.cat(all_targets, dim=0)\n\nprint(f\"\\nEvaluation set: {all_predictions.shape[0]} samples\")\n\n# Compute metrics\nmse = nn.MSELoss()(all_predictions, all_targets).item()\nmae = nn.L1Loss()(all_predictions, all_targets).item()\n\nprint(f\"\\nMetrics:\")\nprint(f\"  MSE: {mse:.6f}\")\nprint(f\"  MAE: {mae:.6f}\")\nprint(f\"  RMSE: {np.sqrt(mse):.6f}\")\n\n# Save metrics\nmetrics = {\n    'mse': mse,\n    'mae': mae,\n    'rmse': np.sqrt(mse),\n    'best_epoch': checkpoint['epoch'] + 1,\n    'best_val_loss': checkpoint['val_loss'],\n}\n\nwith open(f\"{CHECKPOINT_DIR}/metrics.json\", 'w') as f:\n    json.dump(metrics, f, indent=2)\n\nprint(f\"\\n✅ Metrics saved to: {CHECKPOINT_DIR}/metrics.json\")\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 8: Generate Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"GENERATING SAMPLE PREDICTIONS\")\nprint(\"=\"*70)\n\n# Select 5 random validation samples\nnum_samples = min(5, len(val_dataset))\nsample_indices = np.random.choice(len(val_dataset), num_samples, replace=False)\n\nfor idx, sample_idx in enumerate(sample_indices):\n    print(f\"\\nGenerating prediction {idx+1}/{num_samples}...\")\n    \n    inputs, outputs_dict = val_dataset[sample_idx]\n    targets = outputs_dict['vil']\n    \n    # Add batch dimension\n    inputs_batch = {k: v.unsqueeze(0).to(device) for k, v in inputs.items()}\n    \n    # Generate prediction (unpack tuple!)\n    with torch.no_grad():\n        predictions, _, _ = model(inputs_batch)\n    \n    # Move to CPU\n    predictions = predictions.cpu().squeeze(0).numpy()\n    targets = targets.numpy()\n    \n    # Visualize three frames: beginning, middle, end\n    frames_to_plot = [0, 5, 11]\n    \n    fig, axes = plt.subplots(len(frames_to_plot), 3, figsize=(18, 5*len(frames_to_plot)))\n    \n    for row_idx, frame_idx in enumerate(frames_to_plot):\n        # Input (VIL) - use last input frame\n        vil_input = inputs['vil'][-1].numpy()\n        im0 = axes[row_idx, 0].imshow(vil_input, cmap='turbo', vmin=0, vmax=1)\n        axes[row_idx, 0].set_title(f'Input VIL (t=11, last input)')\n        axes[row_idx, 0].axis('off')\n        plt.colorbar(im0, ax=axes[row_idx, 0], fraction=0.046, pad=0.04)\n        \n        # Prediction\n        im1 = axes[row_idx, 1].imshow(predictions[frame_idx], cmap='turbo', vmin=0, vmax=1)\n        axes[row_idx, 1].set_title(f'Prediction (t={12+frame_idx})')\n        axes[row_idx, 1].axis('off')\n        plt.colorbar(im1, ax=axes[row_idx, 1], fraction=0.046, pad=0.04)\n        \n        # Ground truth\n        im2 = axes[row_idx, 2].imshow(targets[frame_idx], cmap='turbo', vmin=0, vmax=1)\n        axes[row_idx, 2].set_title(f'Ground Truth (t={12+frame_idx})')\n        axes[row_idx, 2].axis('off')\n        plt.colorbar(im2, ax=axes[row_idx, 2], fraction=0.046, pad=0.04)\n    \n    plt.tight_layout()\n    plt.savefig(f\"{CHECKPOINT_DIR}/sample_prediction_{idx+1}.png\", dpi=150, bbox_inches='tight')\n    plt.close()\n    \n    print(f\"  ✅ Saved to: {CHECKPOINT_DIR}/sample_prediction_{idx+1}.png\")\n\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Training completed successfully!**\n",
    "\n",
    "**Final results:**\n",
    "- Best validation loss: `{best_val_loss:.6f}`\n",
    "- Best epoch: `{checkpoint['epoch']+1}`\n",
    "- Total training time: `{sum(history['epoch_times'])/3600:.2f}` hours\n",
    "\n",
    "**Files saved:**\n",
    "```\n",
    "{CHECKPOINT_DIR}/\n",
    "├── config.json                 # Training configuration\n",
    "├── history.json                # Loss/metric history\n",
    "├── metrics.json                # Final evaluation metrics\n",
    "├── best_model.pt               # Best model checkpoint\n",
    "├── latest_checkpoint.pt        # Latest checkpoint\n",
    "├── checkpoint_epoch_*.pt       # Periodic checkpoints\n",
    "├── training_curves.png         # Training visualizations\n",
    "└── sample_prediction_*.png     # Sample predictions\n",
    "```\n",
    "\n",
    "**Next steps for Paper 1:**\n",
    "\n",
    "1. **Compute CSI metrics:** Critical Severity Index at thresholds [16, 74, 133, 160, 181, 219]\n",
    "2. **Run ablation studies:** Test without GNN, Transformer, Physics loss\n",
    "3. **Compare with baselines:** ConvLSTM, U-Net, Persistence\n",
    "4. **Generate paper figures:** Create visualizations for publication\n",
    "5. **Write results section:** Document findings in paper\n",
    "\n",
    "---\n",
    "\n",
    "**For Paper 2 (Stage 5):**\n",
    "- Use this trained model as baseline\n",
    "- Add perceptual loss (LPIPS)\n",
    "- Add GAN discriminator\n",
    "- That will be a separate notebook series"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}