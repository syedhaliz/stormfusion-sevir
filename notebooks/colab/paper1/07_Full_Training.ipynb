{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 07: Full-Scale Training (Paper 1 - Storm-Graph Transformer)\n",
    "\n",
    "**Purpose:** Train SGT model on full SEVIR dataset for Paper 1\n",
    "\n",
    "**What this does:**\n",
    "- Train on full dataset (hundreds of events)\n",
    "- Run for many epochs (20-50)\n",
    "- Track metrics: MSE, MAE, CSI\n",
    "- Save checkpoints regularly\n",
    "- Generate evaluation visualizations\n",
    "- Compare with baseline methods\n",
    "\n",
    "**What this does NOT do:**\n",
    "- Use Stage 5 techniques (that's Paper 2)\n",
    "- Run ablation studies (create separate notebook if needed)\n",
    "\n",
    "**Expected time:** Several hours to days (depending on dataset size)\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:** \n",
    "- Run `01_Setup_and_Environment.ipynb` first\n",
    "- Run `02_Data_Verification.ipynb` and download FULL dataset\n",
    "- Run `06_Small_Scale_Training.ipynb` to verify training works\n",
    "- Ensure you have sufficient GPU memory and disk space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\nimport sys\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport json\nfrom datetime import datetime\n\n# Mount Drive\nprint(\"Mounting Google Drive...\")\ndrive.mount('/content/drive', force_remount=False)\nprint(\"✅ Drive mounted\\n\")\n\n# Clone/update repository\nREPO_PATH = '/content/stormfusion-sevir'\nif not os.path.exists(REPO_PATH):\n    print(\"Cloning repository...\")\n    !git clone https://github.com/syedhaliz/stormfusion-sevir.git {REPO_PATH}\n    print(\"✅ Repository cloned\\n\")\nelse:\n    print(\"Repository exists, pulling latest changes...\")\n    !cd {REPO_PATH} && git pull\n    print(\"✅ Repository updated\\n\")\n\n# Add repository to path\nif REPO_PATH not in sys.path:\n    sys.path.insert(0, REPO_PATH)\n    print(f\"✅ Added {REPO_PATH} to Python path\\n\")\n\n# Force reload of modules to get latest code\nimport importlib\nfor module_name in ['stormfusion.models.sgt', 'stormfusion.data.sevir_multimodal']:\n    if module_name in sys.modules:\n        importlib.reload(sys.modules[module_name])\n        \nprint(\"✅ Modules reloaded\\n\")\n\n# Paths\nDRIVE_ROOT = \"/content/drive/MyDrive/SEVIR_Data\"\nSEVIR_ROOT = f\"{DRIVE_ROOT}/data/sevir\"\nCATALOG_PATH = f\"{DRIVE_ROOT}/data/SEVIR_CATALOG.csv\"\n\n# Create experiment directory with timestamp\nEXPERIMENT_NAME = f\"sgt_full_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\nCHECKPOINT_DIR = f\"{DRIVE_ROOT}/experiments/{EXPERIMENT_NAME}\"\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\nprint(f\"Experiment: {EXPERIMENT_NAME}\")\nprint(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\nprint(f\"\\nPyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\\n\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    # Model\n",
    "    'modalities': ['vil', 'ir069', 'ir107', 'lght'],\n",
    "    'input_steps': 12,\n",
    "    'output_steps': 12,\n",
    "    'base_channels': 64,\n",
    "    \n",
    "    # Training\n",
    "    'num_epochs': 30,\n",
    "    'batch_size': 4,\n",
    "    'num_workers': 4,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'grad_clip': 1.0,\n",
    "    \n",
    "    # Scheduler\n",
    "    'scheduler_patience': 3,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'min_lr': 1e-7,\n",
    "    \n",
    "    # Data\n",
    "    'train_augment': True,\n",
    "    'max_train_events': None,  # None = use all\n",
    "    'max_val_events': 100,     # Limit validation for speed\n",
    "    \n",
    "    # Checkpointing\n",
    "    'save_every': 5,  # Save checkpoint every N epochs\n",
    "    'keep_last_n': 3,  # Keep only last N checkpoints\n",
    "}\n",
    "\n",
    "# Save config\n",
    "with open(f\"{CHECKPOINT_DIR}/config.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 3: Load Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stormfusion.data.sevir_multimodal import SEVIRMultiModalDataset\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING FULL DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load catalog\n",
    "catalog = pd.read_csv(CATALOG_PATH, low_memory=False)\n",
    "print(f\"\\nCatalog loaded: {len(catalog)} entries\")\n",
    "\n",
    "# Get train/val splits\n",
    "all_train_events = catalog[catalog['split'] == 'train']['id'].unique()\n",
    "all_val_events = catalog[catalog['split'] == 'val']['id'].unique()\n",
    "\n",
    "print(f\"\\nFull dataset:\")\n",
    "print(f\"  Total train events: {len(all_train_events)}\")\n",
    "print(f\"  Total val events: {len(all_val_events)}\")\n",
    "\n",
    "# Apply limits if specified\n",
    "if config['max_train_events'] is not None:\n",
    "    train_events = all_train_events[:config['max_train_events']]\n",
    "    print(f\"  Using train events: {len(train_events)} (limited)\")\n",
    "else:\n",
    "    train_events = all_train_events\n",
    "    print(f\"  Using train events: {len(train_events)} (all)\")\n",
    "\n",
    "if config['max_val_events'] is not None:\n",
    "    val_events = all_val_events[:config['max_val_events']]\n",
    "    print(f\"  Using val events: {len(val_events)} (limited)\")\n",
    "else:\n",
    "    val_events = all_val_events\n",
    "    print(f\"  Using val events: {len(val_events)} (all)\")\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nCreating datasets...\")\n",
    "train_dataset = SEVIRMultiModalDataset(\n",
    "    catalog=catalog,\n",
    "    data_root=SEVIR_ROOT,\n",
    "    event_ids=train_events,\n",
    "    input_steps=config['input_steps'],\n",
    "    output_steps=config['output_steps'],\n",
    "    modalities=config['modalities'],\n",
    "    augment=config['train_augment']\n",
    ")\n",
    "\n",
    "val_dataset = SEVIRMultiModalDataset(\n",
    "    catalog=catalog,\n",
    "    data_root=SEVIR_ROOT,\n",
    "    event_ids=val_events,\n",
    "    input_steps=config['input_steps'],\n",
    "    output_steps=config['output_steps'],\n",
    "    modalities=config['modalities'],\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val: {len(val_dataset)} samples\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataloader info:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Batch size: {config['batch_size']}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 4: Create Model and Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stormfusion.models.sgt import create_sgt_model\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING MODEL AND TRAINING COMPONENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create model\n",
    "model = create_sgt_model(\n",
    "    modalities=config['modalities'],\n",
    "    input_steps=config['input_steps'],\n",
    "    output_steps=config['output_steps'],\n",
    "    base_channels=config['base_channels']\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel created: {total_params:,} parameters ({total_params/1e6:.2f}M)\")\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=config['scheduler_factor'],\n",
    "    patience=config['scheduler_patience'],\n",
    "    min_lr=config['min_lr'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Loss criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"\\n✅ All components created\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STARTING FULL-SCALE TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_mse': [],\n",
    "    'val_mse': [],\n",
    "    'lr': [],\n",
    "    'epoch_times': []\n",
    "}\n",
    "\n",
    "import time\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # ============= TRAINING =============\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_mse = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        mse_loss = criterion(outputs, targets)\n",
    "        loss = mse_loss\n",
    "        \n",
    "        # Add physics loss if available\n",
    "        if hasattr(model, 'physics_loss'):\n",
    "            physics_loss = model.physics_loss(outputs, targets)\n",
    "            loss = loss + physics_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['grad_clip'])\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        train_loss += loss.item()\n",
    "        train_mse += mse_loss.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.6f}',\n",
    "            'mse': f'{mse_loss.item():.6f}'\n",
    "        })\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_train_mse = train_mse / len(train_loader)\n",
    "    \n",
    "    # ============= VALIDATION =============\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_mse = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader, desc='Validation'):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            mse_loss = criterion(outputs, targets)\n",
    "            loss = mse_loss\n",
    "            \n",
    "            if hasattr(model, 'physics_loss'):\n",
    "                physics_loss = model.physics_loss(outputs, targets)\n",
    "                loss = loss + physics_loss\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_mse += mse_loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_mse = val_mse / len(val_loader)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(avg_val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Compute epoch time\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['train_mse'].append(avg_train_mse)\n",
    "    history['val_mse'].append(avg_val_mse)\n",
    "    history['lr'].append(current_lr)\n",
    "    history['epoch_times'].append(epoch_time)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.6f}\")\n",
    "    print(f\"  Val Loss:   {avg_val_loss:.6f}\")\n",
    "    print(f\"  Train MSE:  {avg_train_mse:.6f}\")\n",
    "    print(f\"  Val MSE:    {avg_val_mse:.6f}\")\n",
    "    print(f\"  LR:         {current_lr:.2e}\")\n",
    "    print(f\"  Time:       {epoch_time/60:.1f} min\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'config': config,\n",
    "        'history': history,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss,\n",
    "    }\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(checkpoint, f\"{CHECKPOINT_DIR}/best_model.pt\")\n",
    "        print(f\"  ✅ Saved best model (val_loss: {best_val_loss:.6f})\")\n",
    "    \n",
    "    # Save latest checkpoint\n",
    "    torch.save(checkpoint, f\"{CHECKPOINT_DIR}/latest_checkpoint.pt\")\n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if (epoch + 1) % config['save_every'] == 0:\n",
    "        torch.save(checkpoint, f\"{CHECKPOINT_DIR}/checkpoint_epoch_{epoch+1}.pt\")\n",
    "        print(f\"  💾 Saved checkpoint at epoch {epoch+1}\")\n",
    "    \n",
    "    # Save history\n",
    "    with open(f\"{CHECKPOINT_DIR}/history.json\", 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBest validation loss: {best_val_loss:.6f}\")\n",
    "print(f\"Total training time: {sum(history['epoch_times'])/3600:.2f} hours\")\n",
    "print(f\"\\nAll results saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 6: Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Loss curve\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o', markersize=4)\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s', markersize=4)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# MSE curve\n",
    "axes[0, 1].plot(history['train_mse'], label='Train MSE', marker='o', markersize=4)\n",
    "axes[0, 1].plot(history['val_mse'], label='Val MSE', marker='s', markersize=4)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('MSE')\n",
    "axes[0, 1].set_title('Mean Squared Error')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 0].plot(history['lr'], marker='o', color='green', markersize=4)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].set_title('Learning Rate Schedule')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Epoch time\n",
    "epoch_times_min = [t/60 for t in history['epoch_times']]\n",
    "axes[1, 1].plot(epoch_times_min, marker='o', color='purple', markersize=4)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Time (minutes)')\n",
    "axes[1, 1].set_title('Training Time per Epoch')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CHECKPOINT_DIR}/training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Training curves saved to: {CHECKPOINT_DIR}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EVALUATING BEST MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(f\"{CHECKPOINT_DIR}/best_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nLoaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"Validation loss: {checkpoint['val_loss']:.6f}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nComputing metrics on validation set...\")\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(val_loader, desc='Evaluating'):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        all_predictions.append(outputs.cpu())\n",
    "        all_targets.append(targets.cpu())\n",
    "\n",
    "# Concatenate\n",
    "all_predictions = torch.cat(all_predictions, dim=0)\n",
    "all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "print(f\"\\nEvaluation set: {all_predictions.shape[0]} samples\")\n",
    "\n",
    "# Compute metrics\n",
    "mse = nn.MSELoss()(all_predictions, all_targets).item()\n",
    "mae = nn.L1Loss()(all_predictions, all_targets).item()\n",
    "\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  MSE: {mse:.6f}\")\n",
    "print(f\"  MAE: {mae:.6f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mse):.6f}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics = {\n",
    "    'mse': mse,\n",
    "    'mae': mae,\n",
    "    'rmse': np.sqrt(mse),\n",
    "    'best_epoch': checkpoint['epoch'] + 1,\n",
    "    'best_val_loss': checkpoint['val_loss'],\n",
    "}\n",
    "\n",
    "with open(f\"{CHECKPOINT_DIR}/metrics.json\", 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Metrics saved to: {CHECKPOINT_DIR}/metrics.json\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 8: Generate Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GENERATING SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select 5 random validation samples\n",
    "num_samples = min(5, len(val_dataset))\n",
    "sample_indices = np.random.choice(len(val_dataset), num_samples, replace=False)\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices):\n",
    "    print(f\"\\nGenerating prediction {idx+1}/{num_samples}...\")\n",
    "    \n",
    "    inputs, targets = val_dataset[sample_idx]\n",
    "    \n",
    "    # Add batch dimension\n",
    "    inputs_batch = {k: v.unsqueeze(0).to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        predictions = model(inputs_batch)\n",
    "    \n",
    "    # Move to CPU\n",
    "    predictions = predictions.cpu().squeeze(0).numpy()\n",
    "    targets = targets.numpy()\n",
    "    \n",
    "    # Visualize three frames: beginning, middle, end\n",
    "    frames_to_plot = [0, 5, 11]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(frames_to_plot), 3, figsize=(18, 5*len(frames_to_plot)))\n",
    "    \n",
    "    for row_idx, frame_idx in enumerate(frames_to_plot):\n",
    "        # Input (VIL)\n",
    "        vil_input = inputs['vil'][frame_idx].numpy()\n",
    "        im0 = axes[row_idx, 0].imshow(vil_input, cmap='turbo', vmin=0, vmax=255)\n",
    "        axes[row_idx, 0].set_title(f'Input VIL (t={frame_idx})')\n",
    "        axes[row_idx, 0].axis('off')\n",
    "        plt.colorbar(im0, ax=axes[row_idx, 0], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Prediction\n",
    "        im1 = axes[row_idx, 1].imshow(predictions[frame_idx], cmap='turbo', vmin=0, vmax=255)\n",
    "        axes[row_idx, 1].set_title(f'Prediction (t={frame_idx+12})')\n",
    "        axes[row_idx, 1].axis('off')\n",
    "        plt.colorbar(im1, ax=axes[row_idx, 1], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Ground truth\n",
    "        im2 = axes[row_idx, 2].imshow(targets[frame_idx], cmap='turbo', vmin=0, vmax=255)\n",
    "        axes[row_idx, 2].set_title(f'Ground Truth (t={frame_idx+12})')\n",
    "        axes[row_idx, 2].axis('off')\n",
    "        plt.colorbar(im2, ax=axes[row_idx, 2], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CHECKPOINT_DIR}/sample_prediction_{idx+1}.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  ✅ Saved to: {CHECKPOINT_DIR}/sample_prediction_{idx+1}.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Training completed successfully!**\n",
    "\n",
    "**Final results:**\n",
    "- Best validation loss: `{best_val_loss:.6f}`\n",
    "- Best epoch: `{checkpoint['epoch']+1}`\n",
    "- Total training time: `{sum(history['epoch_times'])/3600:.2f}` hours\n",
    "\n",
    "**Files saved:**\n",
    "```\n",
    "{CHECKPOINT_DIR}/\n",
    "├── config.json                 # Training configuration\n",
    "├── history.json                # Loss/metric history\n",
    "├── metrics.json                # Final evaluation metrics\n",
    "├── best_model.pt               # Best model checkpoint\n",
    "├── latest_checkpoint.pt        # Latest checkpoint\n",
    "├── checkpoint_epoch_*.pt       # Periodic checkpoints\n",
    "├── training_curves.png         # Training visualizations\n",
    "└── sample_prediction_*.png     # Sample predictions\n",
    "```\n",
    "\n",
    "**Next steps for Paper 1:**\n",
    "\n",
    "1. **Compute CSI metrics:** Critical Severity Index at thresholds [16, 74, 133, 160, 181, 219]\n",
    "2. **Run ablation studies:** Test without GNN, Transformer, Physics loss\n",
    "3. **Compare with baselines:** ConvLSTM, U-Net, Persistence\n",
    "4. **Generate paper figures:** Create visualizations for publication\n",
    "5. **Write results section:** Document findings in paper\n",
    "\n",
    "---\n",
    "\n",
    "**For Paper 2 (Stage 5):**\n",
    "- Use this trained model as baseline\n",
    "- Add perceptual loss (LPIPS)\n",
    "- Add GAN discriminator\n",
    "- That will be a separate notebook series"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}