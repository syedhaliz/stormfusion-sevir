{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# 04: Test Model Components (Individually)\n\n**Purpose:** Test each SGT module in isolation before integration\n\n**What this does:**\n- Test each of the 7 SGT modules independently\n- Verify input/output shapes\n- Check for tensor errors\n- Ensure each component works before assembly\n\n**What this does NOT do:**\n- Load real SEVIR data (uses dummy tensors)\n- Train the model\n- Test the integrated model (that's notebook 05)\n\n**Expected time:** 5-10 minutes\n\n---\n\n## ⚠️ IMPORTANT: How to Run This Notebook\n\n**YOU MUST run cells IN ORDER:**\n1. **FIRST:** Run \"Step 1: Setup\" cell below (mounts Drive, clones repo)\n2. **THEN:** Run the other cells in sequence\n\n**If you skip Step 1, you'll get `ModuleNotFoundError`!**\n\n---\n\n**Prerequisites:** \n- Run `01_Setup_and_Environment.ipynb` first\n- Repository should be cloned and in Python path"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\nimport sys\nimport os\nimport torch\nimport torch.nn as nn\n\n# Mount Drive\nprint(\"Mounting Google Drive...\")\ndrive.mount('/content/drive', force_remount=False)\nprint(\"✅ Drive mounted\\n\")\n\n# Install dependencies (each Colab session needs this!)\nprint(\"Installing dependencies...\")\n!pip install -q torch-geometric h5py pandas tqdm matplotlib lpips scikit-image scipy\nprint(\"✅ Dependencies installed\\n\")\n\n# Clone/update repository\nREPO_PATH = '/content/stormfusion-sevir'\nif not os.path.exists(REPO_PATH):\n    print(\"Cloning repository...\")\n    !git clone https://github.com/syedhaliz/stormfusion-sevir.git {REPO_PATH}\n    print(\"✅ Repository cloned\\n\")\nelse:\n    print(\"Repository exists, pulling latest changes...\")\n    !cd {REPO_PATH} && git pull\n    print(\"✅ Repository updated\\n\")\n\n# Add repo to path\nif REPO_PATH not in sys.path:\n    sys.path.insert(0, REPO_PATH)\n    print(f\"✅ Added {REPO_PATH} to Python path\\n\")\n\n# Force reload of modules to get latest code\nimport importlib\nfor module_name in ['stormfusion.models.sgt.encoder', 'stormfusion.models.sgt.decoder', \n                    'stormfusion.models.sgt.detector', 'stormfusion.models.sgt.gnn',\n                    'stormfusion.models.sgt.transformer', 'stormfusion.models.sgt.physics_loss']:\n    if module_name in sys.modules:\n        importlib.reload(sys.modules[module_name])\n        \nprint(\"✅ Modules reloaded\\n\")\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\\n\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: Test MultiModalEncoder\n",
    "\n",
    "**What it does:** Encodes 4 input modalities into unified representation\n",
    "\n",
    "**Expected:**\n",
    "- Input: `{vil: [B,12,384,384], ir069: [B,12,384,384], ir107: [B,12,384,384], lght: [B,12,384,384]}`\n",
    "- Output: `[B, 256, 24, 24]` (encoded spatial features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Ensure repo is in path (in case you skipped setup cell)\nimport sys\nREPO_PATH = '/content/stormfusion-sevir'\nif REPO_PATH not in sys.path:\n    print(\"⚠️  WARNING: Run Step 1 (Setup) cell first!\")\n    print(\"   Adding repo to path now, but you should run the setup cell.\\n\")\n    sys.path.insert(0, REPO_PATH)\n\nfrom stormfusion.models.sgt.encoder import MultiModalEncoder\nimport torch\n\nprint(\"=\"*70)\nprint(\"TEST 1: MultiModalEncoder\")\nprint(\"=\"*70)\n\n# Create encoder (correct parameters: hidden_dim not base_channels!)\nencoder = MultiModalEncoder(\n    modalities=['vil', 'ir069', 'ir107', 'lght'],\n    input_steps=12,\n    hidden_dim=128  # ← Correct parameter name\n).to(device)\n\n# Count parameters\nencoder_params = sum(p.numel() for p in encoder.parameters())\nprint(f\"\\nEncoder parameters: {encoder_params:,}\")\n\n# Create dummy input (batch_size=2)\nB = 2\ndummy_input = {\n    'vil': torch.randn(B, 12, 384, 384).to(device),\n    'ir069': torch.randn(B, 12, 384, 384).to(device),\n    'ir107': torch.randn(B, 12, 384, 384).to(device),\n    'lght': torch.randn(B, 12, 384, 384).to(device)\n}\n\nprint(f\"\\nInput shapes:\")\nfor mod, tensor in dummy_input.items():\n    print(f\"  {mod:8s}: {tuple(tensor.shape)}\")\n\n# Forward pass\ntry:\n    with torch.no_grad():\n        encoded = encoder(dummy_input)\n    \n    print(f\"\\nOutput shape: {tuple(encoded.shape)}\")\n    print(f\"Expected: ({B}, 128, 96, 96)\")  # hidden_dim=128, 384/4=96\n    \n    if encoded.shape == (B, 128, 96, 96):\n        print(\"\\n✅ Encoder works correctly!\")\n    else:\n        print(f\"\\n⚠️  Shape mismatch: got {tuple(encoded.shape)}\")\n        \nexcept Exception as e:\n    print(f\"\\n❌ Error: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 3: Test StormCellDetector\n",
    "\n",
    "**What it does:** Detects storm cells and converts to graph nodes\n",
    "\n",
    "**Expected:**\n",
    "- Input: `[B, 256, 24, 24]` (encoded features)\n",
    "- Output: Graph with detected nodes (variable number per sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "from stormfusion.models.sgt.detector import StormCellDetector\n\nprint(\"=\"*70)\nprint(\"TEST 2: StormCellDetector\")\nprint(\"=\"*70)\n\n# Create detector (correct parameters!)\ndetector = StormCellDetector(\n    feature_dim=128,  # ← Correct parameter name\n    min_intensity=0.3,\n    min_distance=8,\n    max_storms=50\n).to(device)\n\ndetector_params = sum(p.numel() for p in detector.parameters())\nprint(f\"\\nDetector parameters: {detector_params:,}\")\n\n# Detector needs BOTH encoded features AND vil_input\nprint(f\"\\nInput shapes:\")\nprint(f\"  Encoded features: {tuple(encoded.shape)}\")\nprint(f\"  VIL input: {tuple(dummy_input['vil'].shape)}\")\n\ntry:\n    with torch.no_grad():\n        # Detector needs: forward(features, vil_input)\n        node_features, node_positions, batch_idx = detector(encoded, dummy_input['vil'])\n    \n    print(f\"\\nDetector output:\")\n    print(f\"  Number of batches: {len(node_features)}\")\n    print(f\"  Batch 0 - nodes: {node_features[0].shape[0]}, features: {node_features[0].shape[1]}\")\n    print(f\"  Batch 1 - nodes: {node_features[1].shape[0]}, features: {node_features[1].shape[1]}\")\n    print(f\"  Batch index shape: {batch_idx.shape}\")\n    \n    print(\"\\n✅ Detector works correctly!\")\n    \nexcept Exception as e:\n    print(f\"\\n❌ Error: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 4: Test StormGNN\n",
    "\n",
    "**What it does:** Updates node features via message passing\n",
    "\n",
    "**Expected:**\n",
    "- Input: Graph with node features `[N, 128]`\n",
    "- Output: Graph with updated features `[N, 128]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "from stormfusion.models.sgt.gnn import StormGNN, StormGraphBuilder\n\nprint(\"=\"*70)\nprint(\"TEST 3: StormGNN\")\nprint(\"=\"*70)\n\n# Create graph builder\ngraph_builder = StormGraphBuilder(k_neighbors=8)\n\n# Build graph from detector output\ngraph_data = graph_builder.build_graph(node_positions, node_features, batch_idx)\n\nprint(f\"\\nGraph constructed:\")\nprint(f\"  Nodes: {graph_data.x.shape[0]}\")\nprint(f\"  Features per node: {graph_data.x.shape[1]}\")\nprint(f\"  Edges: {graph_data.edge_index.shape[1]}\")\n\n# Create GNN\ngnn = StormGNN(\n    hidden_dim=128,\n    num_layers=3,\n    num_heads=4,\n    dropout=0.1\n).to(device)\n\ngnn_params = sum(p.numel() for p in gnn.parameters())\nprint(f\"\\nGNN parameters: {gnn_params:,}\")\n\ntry:\n    with torch.no_grad():\n        # Move graph to device\n        graph_data = graph_data.to(device)\n        \n        # Apply GNN\n        updated_features, attention_weights = gnn(graph_data)\n    \n    print(f\"\\nGNN output:\")\n    print(f\"  Updated features shape: {tuple(updated_features.shape)}\")\n    print(f\"  Attention layers: {len(attention_weights)}\")\n    \n    # Should preserve shape\n    if updated_features.shape == graph_data.x.shape:\n        print(\"\\n✅ GNN works correctly!\")\n    else:\n        print(f\"\\n⚠️  Shape changed unexpectedly\")\n    \nexcept Exception as e:\n    print(f\"\\n❌ Error: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 5: Test SpatioTemporalTransformer\n",
    "\n",
    "**What it does:** Applies attention over spatial patches and temporal steps\n",
    "\n",
    "**Expected:**\n",
    "- Input: `[B, 256, 24, 24]` (spatial features) + graph context\n",
    "- Output: `[B, 256, 24, 24]` (attended features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stormfusion.models.sgt.transformer import SpatioTemporalTransformer\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 4: SpatioTemporalTransformer\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create transformer\n",
    "transformer = SpatioTemporalTransformer(\n",
    "    dim=256,\n",
    "    depth=4,\n",
    "    heads=8,\n",
    "    mlp_dim=1024,\n",
    "    patch_size=4\n",
    ").to(device)\n",
    "\n",
    "transformer_params = sum(p.numel() for p in transformer.parameters())\n",
    "print(f\"\\nTransformer parameters: {transformer_params:,}\")\n",
    "\n",
    "# Use encoded features\n",
    "print(f\"\\nInput shape: {tuple(encoded.shape)}\")\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # Transformer takes spatial features + optional graph context\n",
    "        attended = transformer(encoded, graph_features=updated_graph.x if hasattr(updated_graph, 'x') else None)\n",
    "    \n",
    "    print(f\"Output shape: {tuple(attended.shape)}\")\n",
    "    print(f\"Expected: {tuple(encoded.shape)}\")\n",
    "    \n",
    "    if attended.shape == encoded.shape:\n",
    "        print(\"\\n✅ Transformer works correctly!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  Shape mismatch\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 6: Test PhysicsDecoder\n",
    "\n",
    "**What it does:** Decodes features into future predictions with physics constraints\n",
    "\n",
    "**Expected:**\n",
    "- Input: `[B, 256, 24, 24]` (attended features)\n",
    "- Output: `[B, 12, 384, 384]` (predicted future frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stormfusion.models.sgt.decoder import PhysicsDecoder\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 5: PhysicsDecoder\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create decoder\n",
    "decoder = PhysicsDecoder(\n",
    "    in_channels=256,\n",
    "    base_channels=64,\n",
    "    output_steps=12\n",
    ").to(device)\n",
    "\n",
    "decoder_params = sum(p.numel() for p in decoder.parameters())\n",
    "print(f\"\\nDecoder parameters: {decoder_params:,}\")\n",
    "\n",
    "# Use attended features\n",
    "print(f\"\\nInput shape: {tuple(attended.shape)}\")\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        predictions = decoder(attended)\n",
    "    \n",
    "    print(f\"Output shape: {tuple(predictions.shape)}\")\n",
    "    print(f\"Expected: ({B}, 12, 384, 384)\")\n",
    "    \n",
    "    # Check statistics\n",
    "    print(f\"\\nOutput statistics:\")\n",
    "    print(f\"  Min: {predictions.min().item():.4f}\")\n",
    "    print(f\"  Max: {predictions.max().item():.4f}\")\n",
    "    print(f\"  Mean: {predictions.mean().item():.4f}\")\n",
    "    print(f\"  Std: {predictions.std().item():.4f}\")\n",
    "    \n",
    "    if predictions.shape == (B, 12, 384, 384):\n",
    "        print(\"\\n✅ Decoder works correctly!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  Shape mismatch\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 7: Test ConservationLoss\n",
    "\n",
    "**What it does:** Physics-informed loss for conservation of mass/energy\n",
    "\n",
    "**Expected:**\n",
    "- Input: predictions `[B, 12, 384, 384]`, targets `[B, 12, 384, 384]`\n",
    "- Output: scalar loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stormfusion.models.sgt.physics_loss import ConservationLoss\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 6: ConservationLoss\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create physics loss\n",
    "physics_loss = ConservationLoss(weight=0.1).to(device)\n",
    "\n",
    "print(f\"\\nLoss weight: {physics_loss.weight}\")\n",
    "\n",
    "# Create dummy target\n",
    "dummy_target = torch.randn(B, 12, 384, 384).to(device)\n",
    "\n",
    "print(f\"\\nPredictions shape: {tuple(predictions.shape)}\")\n",
    "print(f\"Targets shape: {tuple(dummy_target.shape)}\")\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        loss = physics_loss(predictions, dummy_target)\n",
    "    \n",
    "    print(f\"\\nLoss value: {loss.item():.6f}\")\n",
    "    print(f\"Loss shape: {loss.shape}\")\n",
    "    \n",
    "    if loss.dim() == 0 and loss.item() >= 0:\n",
    "        print(\"\\n✅ Physics loss works correctly!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  Unexpected loss value or shape\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we tested:**\n",
    "- ✅ MultiModalEncoder: 4 modalities → unified representation\n",
    "- ✅ StormCellDetector: spatial features → graph nodes\n",
    "- ✅ StormGNN: message passing on storm cell graph\n",
    "- ✅ SpatioTemporalTransformer: attention over patches\n",
    "- ✅ PhysicsDecoder: features → future predictions\n",
    "- ✅ ConservationLoss: physics-informed constraints\n",
    "\n",
    "**Parameter counts:**\n",
    "- Encoder: ~XXX K\n",
    "- Detector: ~XXX K\n",
    "- GNN: ~XXX K\n",
    "- Transformer: ~XXX M (largest component)\n",
    "- Decoder: ~XXX K\n",
    "\n",
    "**Next steps:**\n",
    "1. If all tests passed ✅, proceed to `05_Test_Full_Model.ipynb`\n",
    "2. That notebook will test the integrated SGT model\n",
    "3. Then we'll move to small-scale training\n",
    "\n",
    "---\n",
    "\n",
    "**If any test failed:**\n",
    "- Check the error traceback\n",
    "- Verify shapes match expected\n",
    "- Check for missing dependencies\n",
    "- Report issues with specific error messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}